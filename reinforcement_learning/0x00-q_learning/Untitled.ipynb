{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!touch 4-play.py\n",
    "!chmod +x *.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\" Load the Environment \"\"\"\n",
    "import gym\n",
    "\n",
    "def load_frozen_lake(desc=None, map_name=None, is_slippery=False):\n",
    "    \"\"\"\n",
    "    ****************************************\n",
    "    ****loads the pre-made FrozenLakeEnv****\n",
    "    *****evnironment from OpenAIâ€™s gym******\n",
    "    ****************************************\n",
    "    @desc: is either None or a list of lists containing a custom\n",
    "           description of the map to load for the environment\n",
    "    @map_name: is either None or a string containing the pre-made\n",
    "               map to load\n",
    "    *** If both desc and map_name are None, the environment will\n",
    "        load a randomly generated 8x8 map\n",
    "    @is_slippery: is a boolean to determine if the ice is slippery\n",
    "    Returns:\n",
    "            the environment\n",
    "    \"\"\"\n",
    "    return gym.make(\"FrozenLake-v0\",\n",
    "                    desc=desc,\n",
    "                    map_name=map_name,\n",
    "                    is_slippery=is_slippery)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[b'S' b'F' b'F' b'F' b'F' b'F' b'F' b'H']\n",
      " [b'H' b'F' b'F' b'F' b'F' b'H' b'F' b'F']\n",
      " [b'F' b'H' b'F' b'H' b'H' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'F' b'H' b'F' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'F' b'F' b'F' b'F' b'H' b'F']\n",
      " [b'F' b'F' b'F' b'F' b'F' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'F' b'F' b'H' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'F' b'F' b'F' b'F' b'F' b'G']]\n",
      "[(1.0, 0, 0.0, False)]\n",
      "[[b'S' b'F' b'H' b'F' b'H' b'F' b'H' b'F']\n",
      " [b'H' b'F' b'F' b'F' b'F' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'F' b'F' b'F' b'F' b'F' b'F']\n",
      " [b'F' b'H' b'F' b'F' b'F' b'F' b'F' b'F']\n",
      " [b'F' b'F' b'H' b'F' b'F' b'F' b'F' b'H']\n",
      " [b'F' b'F' b'F' b'F' b'F' b'H' b'F' b'H']\n",
      " [b'F' b'F' b'H' b'F' b'H' b'F' b'H' b'F']\n",
      " [b'F' b'F' b'H' b'F' b'F' b'F' b'F' b'G']]\n",
      "[(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 8, 0.0, True)]\n",
      "[[b'S' b'F' b'F']\n",
      " [b'F' b'H' b'H']\n",
      " [b'F' b'F' b'G']]\n",
      "[[b'S' b'F' b'F' b'F']\n",
      " [b'F' b'H' b'F' b'H']\n",
      " [b'F' b'F' b'F' b'H']\n",
      " [b'H' b'F' b'F' b'G']]\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "env = load_frozen_lake()\n",
    "print(env.desc)\n",
    "print(env.P[0][0])\n",
    "env = load_frozen_lake(is_slippery=True)\n",
    "print(env.desc)\n",
    "print(env.P[0][0])\n",
    "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
    "env = load_frozen_lake(desc=desc)\n",
    "print(env.desc)\n",
    "env = load_frozen_lake(map_name='4x4')\n",
    "print(env.desc)\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khouloud/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(1000): \n",
    "    env.render()\n",
    "    env.step(env.action_space.sample()) # take a random action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on TimeLimit in module gym.wrappers.time_limit object:\n",
      "\n",
      "class TimeLimit(gym.core.Wrapper)\n",
      " |  Wraps the environment to allow a modular transformation.\n",
      " |  \n",
      " |  This class is the base class for all wrappers. The subclass could override\n",
      " |  some methods to change the behavior of the original environment without touching the\n",
      " |  original code.\n",
      " |  \n",
      " |  .. note::\n",
      " |  \n",
      " |      Don't forget to call ``super().__init__(env)`` if the subclass overrides :meth:`__init__`.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TimeLimit\n",
      " |      gym.core.Wrapper\n",
      " |      gym.core.Env\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, env, max_episode_steps=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  reset(self, **kwargs)\n",
      " |      Resets the environment to an initial state and returns an initial\n",
      " |      observation.\n",
      " |      \n",
      " |      Note that this function should not reset the environment's random\n",
      " |      number generator(s); random variables in the environment's state should\n",
      " |      be sampled independently between multiple calls to `reset()`. In other\n",
      " |      words, each call of `reset()` should yield an environment suitable for\n",
      " |      a new episode, independent of previous episodes.\n",
      " |      \n",
      " |      Returns:\n",
      " |          observation (object): the initial observation.\n",
      " |  \n",
      " |  step(self, action)\n",
      " |      Run one timestep of the environment's dynamics. When end of\n",
      " |      episode is reached, you are responsible for calling `reset()`\n",
      " |      to reset this environment's state.\n",
      " |      \n",
      " |      Accepts an action and returns a tuple (observation, reward, done, info).\n",
      " |      \n",
      " |      Args:\n",
      " |          action (object): an action provided by the agent\n",
      " |      \n",
      " |      Returns:\n",
      " |          observation (object): agent's observation of the current environment\n",
      " |          reward (float) : amount of reward returned after previous action\n",
      " |          done (bool): whether the episode has ended, in which case further step() calls will return undefined results\n",
      " |          info (dict): contains auxiliary diagnostic information (helpful for debugging, and sometimes learning)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gym.core.Wrapper:\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  close(self)\n",
      " |      Override close in your subclass to perform any necessary cleanup.\n",
      " |      \n",
      " |      Environments will automatically close() themselves when\n",
      " |      garbage collected or when the program exits.\n",
      " |  \n",
      " |  compute_reward(self, achieved_goal, desired_goal, info)\n",
      " |  \n",
      " |  render(self, mode='human', **kwargs)\n",
      " |      Renders the environment.\n",
      " |      \n",
      " |      The set of supported modes varies per environment. (And some\n",
      " |      environments do not support rendering at all.) By convention,\n",
      " |      if mode is:\n",
      " |      \n",
      " |      - human: render to the current display or terminal and\n",
      " |        return nothing. Usually for human consumption.\n",
      " |      - rgb_array: Return an numpy.ndarray with shape (x, y, 3),\n",
      " |        representing RGB values for an x-by-y pixel image, suitable\n",
      " |        for turning into a video.\n",
      " |      - ansi: Return a string (str) or StringIO.StringIO containing a\n",
      " |        terminal-style text representation. The text can include newlines\n",
      " |        and ANSI escape sequences (e.g. for colors).\n",
      " |      \n",
      " |      Note:\n",
      " |          Make sure that your class's metadata 'render.modes' key includes\n",
      " |            the list of supported modes. It's recommended to call super()\n",
      " |            in implementations to use the functionality of this method.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (str): the mode to render with\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      class MyEnv(Env):\n",
      " |          metadata = {'render.modes': ['human', 'rgb_array']}\n",
      " |      \n",
      " |          def render(self, mode='human'):\n",
      " |              if mode == 'rgb_array':\n",
      " |                  return np.array(...) # return RGB frame suitable for video\n",
      " |              elif mode == 'human':\n",
      " |                  ... # pop up a window and render\n",
      " |              else:\n",
      " |                  super(MyEnv, self).render(mode=mode) # just raise an exception\n",
      " |  \n",
      " |  seed(self, seed=None)\n",
      " |      Sets the seed for this env's random number generator(s).\n",
      " |      \n",
      " |      Note:\n",
      " |          Some environments use multiple pseudorandom number generators.\n",
      " |          We want to capture all such seeds used in order to ensure that\n",
      " |          there aren't accidental correlations between multiple generators.\n",
      " |      \n",
      " |      Returns:\n",
      " |          list<bigint>: Returns the list of seeds used in this env's random\n",
      " |            number generators. The first value in the list should be the\n",
      " |            \"main\" seed, or the value which a reproducer should pass to\n",
      " |            'seed'. Often, the main seed equals the provided 'seed', but\n",
      " |            this won't be true if seed=None, for example.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from gym.core.Wrapper:\n",
      " |  \n",
      " |  class_name() from builtins.type\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gym.core.Wrapper:\n",
      " |  \n",
      " |  spec\n",
      " |  \n",
      " |  unwrapped\n",
      " |      Completely unwrap this env.\n",
      " |      \n",
      " |      Returns:\n",
      " |          gym.Env: The base non-wrapped gym.Env instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from gym.core.Env:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      Support with-statement for the environment.\n",
      " |  \n",
      " |  __exit__(self, *args)\n",
      " |      Support with-statement for the environment.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gym.core.Env:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from gym.core.Env:\n",
      " |  \n",
      " |  action_space = None\n",
      " |  \n",
      " |  metadata = {'render.modes': []}\n",
      " |  \n",
      " |  observation_space = None\n",
      " |  \n",
      " |  reward_range = (-inf, inf)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(gym.make(\"FrozenLake-v0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\" Initialize Q-table \"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def q_init(env):\n",
    "    \"\"\"\n",
    "    ****************************************\n",
    "    ***********Initialize Q-table***********\n",
    "    ****************************************\n",
    "    @env: is the FrozenLakeEnv instance\n",
    "    Returns:\n",
    "            the Q-table as a numpy.ndarray of zeros\n",
    "    \"\"\"\n",
    "    return np.zeros((env.observation_space.n,\n",
    "                     env.action_space.n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 4)\n",
      "(64, 4)\n",
      "(9, 4)\n",
      "(16, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = load_frozen_lake()\n",
    "Q = q_init(env)\n",
    "print(Q.shape)\n",
    "env = load_frozen_lake(is_slippery=True)\n",
    "Q = q_init(env)\n",
    "print(Q.shape)\n",
    "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
    "env = load_frozen_lake(desc=desc)\n",
    "Q = q_init(env)\n",
    "print(Q.shape)\n",
    "env = load_frozen_lake(map_name='4x4')\n",
    "Q = q_init(env)\n",
    "print(Q.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\" Epsilon Greedy \"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def epsilon_greedy(Q, state, epsilon):\n",
    "    \"\"\"\n",
    "    ******************************************************\n",
    "    ***********uses epsilon-greedy to determine***********\n",
    "    *******************the next action********************\n",
    "    ******************************************************\n",
    "    @Q: is a numpy.ndarray containing the q-table\n",
    "    @state: is the current state\n",
    "    @epsilon: is the epsilon to use for the calculation\n",
    "    *** You should sample p with numpy.random.uniformn to determine\n",
    "        if your algorithm should explore or exploit\n",
    "    *** If exploring, you should pick the next action with\n",
    "        numpy.random.randint from all possible actions\n",
    "    Returns:\n",
    "            the next action index\n",
    "    \"\"\"\n",
    "    p = np.random.uniform()\n",
    "    if p < epsilon:\n",
    "        index = np.random.randint(Q.shape[1])\n",
    "    else:\n",
    "        index = np.argmax(Q[state])\n",
    "\n",
    "    return index\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
    "env = load_frozen_lake(desc=desc)\n",
    "Q = q_init(env)\n",
    "Q[7] = np.array([0.5, 0.7, 1, -1])\n",
    "np.random.seed(0)\n",
    "print(epsilon_greedy(Q, 7, 0.5))\n",
    "np.random.seed(1)\n",
    "print(epsilon_greedy(Q, 7, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\" Q-learning \"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def train(env, Q, episodes=5000, max_steps=100,\n",
    "          alpha=0.1, gamma=0.99, epsilon=1,\n",
    "          min_epsilon=0.1, epsilon_decay=0.05):\n",
    "    \"\"\"\n",
    "    ***********************************************\n",
    "    **************performs Q-learning**************\n",
    "    ***********************************************\n",
    "    @env: is the FrozenLakeEnv instance\n",
    "    @Q: is a numpy.ndarray containing the Q-table\n",
    "    @episodes: is the total number of episodes to train over\n",
    "    @max_steps: is the maximum number of steps per episode\n",
    "    @alpha: is the learning rate\n",
    "    @gamma: is the discount rate\n",
    "    @epsilon: is the initial threshold for epsilon greedy\n",
    "    @min_epsilon: is the minimum value that epsilon should\n",
    "                  decay to\n",
    "    @epsilon_decay: is the decay rate for updating epsilon\n",
    "                    between episodes\n",
    "    *** When the agent falls in a hole, the reward should\n",
    "        be updated to be -1\n",
    "    Returns:\n",
    "            Q: is the updated Q-table\n",
    "            total_rewards: is a list containing the rewards\n",
    "                           per episode\n",
    "    \"\"\"\n",
    "    training_rewards = []  \n",
    "    epsilons = []\n",
    "    max_epsilon = 1\n",
    "    for episode in range(episodes):\n",
    "        # Reseting the environment each time as per requirement\n",
    "        state = env.reset()    \n",
    "        # Starting the tracker for the rewards\n",
    "        total_training_rewards = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # Performing epsilon greedy\n",
    "            action = epsilon_greedy(Q, state, epsilon)\n",
    "  \n",
    "            # Taking the action and getting the reward and outcome state\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # Agent falling in a hole\n",
    "            if done and reward == 0:\n",
    "                reward = -1\n",
    "\n",
    "            # Updating the Q-table using the Bellman equation\n",
    "            Q[state, action] = (Q[state, action] + alpha\n",
    "                                * (reward + gamma * np.max(Q[new_state])\n",
    "                                - Q[state, action]))\n",
    "\n",
    "            # Increasing our total reward and updating the state\n",
    "            total_training_rewards += reward      \n",
    "            state = new_state         \n",
    "\n",
    "            # Ending the episode\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Cutting down on exploration by reducing the epsilon \n",
    "        epsilon = (min_epsilon + (max_epsilon - min_epsilon)\n",
    "                   * np.exp(-epsilon_decay * episode))\n",
    "\n",
    "        # Adding the total reward and reduced epsilon values\n",
    "        training_rewards.append(total_training_rewards)\n",
    "        epsilons.append(epsilon)\n",
    "\n",
    "    return Q, training_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.96059593  0.970299    0.95098488  0.96059396]\n",
      " [ 0.96059557 -0.77123208  0.0094072   0.37627228]\n",
      " [ 0.18061285 -0.1         0.          0.        ]\n",
      " [ 0.97029877  0.9801     -0.99999988  0.96059583]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.98009763  0.98009933  0.99        0.9702983 ]\n",
      " [ 0.98009922  0.98999782  1.         -0.99999952]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "500 : 0.812\n",
      "1000 : 0.88\n",
      "1500 : 0.9\n",
      "2000 : 0.9\n",
      "2500 : 0.88\n",
      "3000 : 0.844\n",
      "3500 : 0.892\n",
      "4000 : 0.896\n",
      "4500 : 0.852\n",
      "5000 : 0.928\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#train = __import__('3-q_learning').train\n",
    "np.random.seed(0)\n",
    "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
    "env = load_frozen_lake(desc=desc)\n",
    "Q = q_init(env)\n",
    "\n",
    "Q, total_rewards  = train(env, Q)\n",
    "print(Q)\n",
    "split_rewards = np.split(np.array(total_rewards), 10)\n",
    "for i, rewards in enumerate(split_rewards):\n",
    "    print((i+1) * 500, ':', np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\" Play \"\"\"\n",
    "import numpy as np\n",
    "\n",
    "epsilon_greedy = __import__('2-epsilon_greedy').epsilon_greedy\n",
    "\n",
    "\n",
    "def play(env, Q, max_steps=100):\n",
    "    \"\"\"\n",
    "    *********************************************\n",
    "    ********trained agent play an episode********\n",
    "    *********************************************\n",
    "    @env: is the FrozenLakeEnv instance\n",
    "    @Q: is a numpy.ndarray containing the Q-table\n",
    "    @max_steps: is the maximum number of steps in\n",
    "                the episode\n",
    "    *** Each state of the board should be displayed\n",
    "        via the console\n",
    "    *** always exploit the Q-table\n",
    "    Returns:\n",
    "            the total rewards for the episode\n",
    "    \"\"\"\n",
    "     # Reseting the environment\n",
    "    state = 0\n",
    "    env.reset()    \n",
    "    env.render()\n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        # Performing epsilon greedy\n",
    "        action = epsilon_greedy(Q, state, 0)\n",
    "        # Taking the action and getting the reward and outcome state\n",
    "        state, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "        # Agent falling in a hole\n",
    "        if done and reward == 0:\n",
    "            return reward      \n",
    "        # Ending the episode\n",
    "        if done:\n",
    "            return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFF\n",
      "FHH\n",
      "FFG\n",
      "  (Down)\n",
      "SFF\n",
      "\u001b[41mF\u001b[0mHH\n",
      "FFG\n",
      "  (Down)\n",
      "SFF\n",
      "FHH\n",
      "\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFF\n",
      "FHH\n",
      "F\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFF\n",
      "FHH\n",
      "FF\u001b[41mG\u001b[0m\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "desc = [['S', 'F', 'F'], ['F', 'H', 'H'], ['F', 'F', 'G']]\n",
    "env = load_frozen_lake(desc=desc)\n",
    "Q = q_init(env)\n",
    "\n",
    "Q, total_rewards  = train(env, Q)\n",
    "print(play(env, Q))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "`S`FF\n",
    "FHH\n",
    "FFG\n",
    "  (Down)\n",
    "SFF\n",
    "`F`HH\n",
    "FFG\n",
    "  (Down)\n",
    "SFF\n",
    "FHH\n",
    "`F`FG\n",
    "  (Right)\n",
    "SFF\n",
    "FHH\n",
    "F`F`G\n",
    "  (Right)\n",
    "SFF\n",
    "FHH\n",
    "FF`G`\n",
    "1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
