{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#value-estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\" Monte Carlo \"\"\"\n",
    "\n",
    "\"\"\" Epsilon Greedy \"\"\"\n",
    "import numpy as np\n",
    "\n",
    "def f_of_x(x):\n",
    "    \"\"\"\n",
    "    This is the main function we want to integrate over.\n",
    "    Args:\n",
    "    - x (float) : input to function; must be in radians\n",
    "    Return:\n",
    "    - output of function f(x) (float)\n",
    "    \"\"\"\n",
    "    return (e**(-1*x))/(1+(x-1)**2)\n",
    "\n",
    "\n",
    "def monte_carlo(env, V, policy, episodes=5000,\n",
    "                max_steps=100, alpha=0.1, gamma=0.99):\n",
    "    \"\"\"\n",
    "    ***********************************************************\n",
    "    *************perform the Monte Carlo algorithm*************\n",
    "    ***********************************************************\n",
    "    @env: is the openAI environment instance\n",
    "    @V: is a numpy.ndarray of shape (s,) containing the value estimate\n",
    "    @policy: is a function that takes in a state and returns the next action to take\n",
    "    @episodes: is the total number of episodes to train over\n",
    "    @max_steps: is the maximum number of steps per episode\n",
    "    @alpha: is the learning rate\n",
    "    @gamma: is the discount rate\n",
    "    Returns: V, the updated value estimate\n",
    "    \"\"\"\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        # Reseting the environment each time as per requirement\n",
    "        state = env.reset()\n",
    "        episode = []\n",
    "        for step in range(max_steps):\n",
    "            action = policy(state)\n",
    "            \n",
    "            # Taking the action and getting the reward and outcome state\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            episode.append([state, action, reward])\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            state = new_state\n",
    "\n",
    "        episode = np.array(episode, dtype=int)\n",
    "        T = len(episode)\n",
    "        G = 0\n",
    "        for t in range(T):\n",
    "            state, action, Returns = episode[t]\n",
    "            G = gamma**t * G + Returns\n",
    "            ##G = sum(gamma**k * episode[t][2] for k in range(T-t-1))\n",
    "            if state not in episode[:ep, 0]:\n",
    "                V[state] = V[state] + alpha * (G - V[state])\n",
    "            \n",
    "\n",
    "    return V\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\" Monte Carlo \"\"\"\n",
    "\n",
    "\"\"\" Epsilon Greedy \"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# all states\n",
    "N_STATES = 19\n",
    "\n",
    "# all states but terminal states\n",
    "STATES = np.arange(1, N_STATES + 1)\n",
    "\n",
    "# start from the middle state\n",
    "START_STATE = 10\n",
    "\n",
    "# two terminal states\n",
    "# an action leading to the left terminal state has reward -1\n",
    "# an action leading to the right terminal state has reward 1\n",
    "END_STATES = [0, N_STATES + 1]\n",
    "\n",
    "# true state values from Bellman equation\n",
    "TRUE_VALUE = np.arange(-20, 22, 2) / 20.0\n",
    "TRUE_VALUE[0] = TRUE_VALUE[N_STATES + 1] = 0.0\n",
    "\n",
    "class ValueFunction:\n",
    "    # @rate: lambda, as it's a keyword in python, so I call it rate\n",
    "    # @stepSize: alpha, step size for update\n",
    "    def __init__(self, rate, step_size, V):\n",
    "        self.rate = rate\n",
    "        self.step_size = step_size\n",
    "        self.weights = V# np.zeros(N_STATES + 2)\n",
    "\n",
    "    # the state value is just the weight\n",
    "    def value(self, state):\n",
    "        return self.weights[state]\n",
    "\n",
    "    # feed the algorithm with new observation\n",
    "    # derived class should override this function\n",
    "    def learn(self, state, reward):\n",
    "        return\n",
    "\n",
    "    # initialize some variables at the beginning of each episode\n",
    "    # must be called at the very beginning of each episode\n",
    "    # derived class should override this function\n",
    "    def new_episode(self):\n",
    "        return\n",
    "\n",
    "class TrueOnlineTemporalDifferenceLambda(ValueFunction):\n",
    "    def __init__(self, rate, step_size, old_state_value, last_state, s, V):\n",
    "        ValueFunction.__init__(self, rate, step_size, V)\n",
    "        # initialize the eligibility trace\n",
    "        self.eligibility = np.zeros(s)\n",
    "        # initialize the beginning state\n",
    "        self.last_state = last_state\n",
    "        # initialize the old state value\n",
    "        self.old_state_value = 0.0\n",
    "\n",
    "    def new_episode(self):\n",
    "        # initialize the eligibility trace\n",
    "        self.eligibility = np.zeros(N_STATES + 2)\n",
    "        # initialize the beginning state\n",
    "        self.last_state = START_STATE\n",
    "        # initialize the old state value\n",
    "        self.old_state_value = 0.0\n",
    "\n",
    "    def learn(self, state, reward):\n",
    "        # update the eligibility trace and weights\n",
    "        last_state_value = self.value(self.last_state)\n",
    "        state_value = self.value(state)\n",
    "        dutch = 1 - self.step_size * self.rate * self.eligibility[self.last_state]\n",
    "        self.eligibility *= self.rate\n",
    "        self.eligibility[self.last_state] += dutch\n",
    "        delta = reward + state_value - last_state_value\n",
    "        self.weights += self.step_size * (delta + last_state_value - self.old_state_value) * self.eligibility\n",
    "        self.weights[self.last_state] -= self.step_size * (last_state_value - self.old_state_value)\n",
    "        self.old_state_value = state_value\n",
    "        self.last_state = state\n",
    "\n",
    "def monte_carlo(env, V, policy, episodes=5000,\n",
    "                max_steps=100, alpha=0.1, gamma=0.99):\n",
    "    \"\"\"\n",
    "    ***********************************************************\n",
    "    *************perform the Monte Carlo algorithm*************\n",
    "    ***********************************************************\n",
    "    @env: is the openAI environment instance\n",
    "    @V: is a numpy.ndarray of shape (s,) containing the value estimate\n",
    "    @policy: is a function that takes in a state and returns\n",
    "             the next action to take\n",
    "    @episodes: is the total number of episodes to train over\n",
    "    @max_steps: is the maximum number of steps per episode\n",
    "    @alpha: is the learning rate\n",
    "    @gamma: is the discount rate\n",
    "    Returns: V, the updated value estimate\n",
    "    \"\"\"\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        # Reseting the environment each time as per requirement\n",
    "        state = env.reset()\n",
    "        episode = []\n",
    "        for step in range(max_steps):\n",
    "            # taking action\n",
    "            action = policy(state)\n",
    "            # Taking the action and getting the reward and outcome state\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            # append results for each state of episode\n",
    "            episode.append([state, action, reward])\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            state = new_state\n",
    "        # Cast and turn episode list to np.ndarray\n",
    "        episode = np.array(episode, dtype=int)\n",
    "        # initiate needed variabes\n",
    "        T = len(episode)  # total number of states starting from 0\n",
    "        G = 0  # empirical return\n",
    "        for t in range(T):\n",
    "            state, action, Returns = episode[t]\n",
    "            # calculate empirical return\n",
    "            G = gamma**t * G + Returns  # summing returns (rewards)\n",
    "            # Value Estimation\n",
    "            if state not in episode[:ep, 0]:\n",
    "                V[state] = V[state] + alpha * (G - V[state])\n",
    "    # Returning the updated Value Estimate\n",
    "    return V\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64,)\n",
      "[[ 0.81  0.9   0.48  0.43  0.39  0.43  0.66  0.9 ]\n",
      " [ 0.9   0.73  0.59  0.48  0.59  0.28  0.28  0.39]\n",
      " [ 1.    0.53  0.73 -1.    1.    0.39  0.28  0.43]\n",
      " [ 1.    0.59  0.81  0.9   1.   -1.    0.39  0.66]\n",
      " [ 1.    0.66  0.81 -1.    1.    1.    0.73  0.53]\n",
      " [ 1.   -1.   -1.    1.    1.    1.   -1.    0.9 ]\n",
      " [ 1.   -1.    1.    1.   -1.    1.   -1.    1.  ]\n",
      " [ 1.    1.    1.   -1.    1.    1.    1.    1.  ]]\n",
      "[[ 0.68  0.62  0.19  0.22  0.39  0.43  0.66  0.9 ]\n",
      " [ 0.71  0.51  0.24 -0.    0.58  0.28  0.28  0.39]\n",
      " [ 0.33  0.41  0.23 -1.    0.91  0.21  0.27  0.38]\n",
      " [ 0.38  0.38  0.39  0.6   0.63 -1.    0.39  0.66]\n",
      " [ 0.65  0.23  0.19 -1.    1.    1.    0.73  0.53]\n",
      " [ 0.62 -1.   -1.    1.    1.    1.   -1.    0.9 ]\n",
      " [ 0.56 -1.    1.    1.   -1.    1.   -1.    1.  ]\n",
      " [ 0.9   1.    1.   -1.    1.    1.    1.    1.  ]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "monte_carlo = __import__('0-monte_carlo').monte_carlo\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "env = gym.make('FrozenLake8x8-v0')\n",
    "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
    "\n",
    "def policy(s):\n",
    "    p = np.random.uniform()\n",
    "    if p > 0.5:\n",
    "        if s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
    "            return RIGHT\n",
    "        elif s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
    "            return DOWN\n",
    "        elif s // 8 != 0 and env.desc[s // 8 - 1, s % 8] != b'H':\n",
    "            return UP\n",
    "        else:\n",
    "            return LEFT\n",
    "    else:\n",
    "        if s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
    "            return DOWN\n",
    "        elif s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
    "            return RIGHT\n",
    "        elif s % 8 != 0 and env.desc[s // 8, s % 8 - 1] != b'H':\n",
    "            return LEFT\n",
    "        else:\n",
    "            return UP\n",
    "\n",
    "V = np.where(env.desc == b'H', -1, 1).reshape(64).astype('float64')\n",
    "print(V.shape)\n",
    "np.set_printoptions(precision=2)\n",
    "env.seed(0)\n",
    "\n",
    "print(monte_carlo(env, V, policy).reshape((8, 8)))\n",
    "print(td_lambtha(env, V, policy, 1).reshape((8, 8)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[ 0.81  0.9   0.48  0.43  0.39  0.43  0.66  0.9 ]\n",
    " [ 0.9   0.73  0.59  0.48  0.59  0.28  0.28  0.39]\n",
    " [ 1.    0.53  0.73 -1.    1.    0.39  0.28  0.43]\n",
    " [ 1.    0.59  0.81  0.9   1.   -1.    0.39  0.66]\n",
    " [ 1.    0.66  0.81 -1.    1.    1.    0.73  0.53]\n",
    " [ 1.   -1.   -1.    1.    1.    1.   -1.    0.9 ]\n",
    " [ 1.   -1.    1.    1.   -1.    1.   -1.    1.  ]\n",
    " [ 1.    1.    1.   -1.    1.    1.    1.    1.  ]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[[ 0.81    0.9     0.4783  0.4305  0.3874  0.4305  0.6561  0.9   ]\n",
    " [ 0.9     0.729   0.5905  0.4783  0.5905  0.2824  0.2824  0.3874]\n",
    " [ 1.      0.5314  0.729  -1.      1.      0.3874  0.2824  0.4305]\n",
    " [ 1.      0.5905  0.81    0.9     1.     -1.      0.3874  0.6561]\n",
    " [ 1.      0.6561  0.81   -1.      1.      1.      0.729   0.5314]\n",
    " [ 1.     -1.     -1.      1.      1.      1.     -1.      0.9   ]\n",
    " [ 1.     -1.      1.      1.     -1.      1.     -1.      1.    ]\n",
    " [ 1.      1.      1.     -1.      1.      1.      1.      1.    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 10000 trials: Win on Swap 3316 - Win when Not swap 1661\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "swap_win = no_swap_win = 0 \n",
    " \n",
    "doors = {'a','b','c'} \n",
    " \n",
    "for trial in range(1, 10001): \n",
    "    # Choose which doors win and which lose \n",
    "    winning = random.choice(list(doors)) \n",
    "    loosing = doors - set(winning) \t  \n",
    " \n",
    "    # Randomly choose which door is picked and which is revealed \n",
    "    picked = random.choice(list(doors))   \n",
    " \n",
    "    # Compere never reveals a winning door \n",
    "    revealed = random.choice(list(loosing - set(picked))) \n",
    "     \n",
    "    # Randomly choose if the contestant swaps \n",
    "    swap = random.choice((True, False)) \n",
    "    if swap: \n",
    "        # If contestants swaps they choose other than the \n",
    "\t\t# one they picked or the compere revealed \n",
    "        picked = random.choice(list(doors - set(picked) - set(revealed))) \n",
    " \n",
    "    # Increment counts \n",
    "    if picked == winning: \n",
    "        if swap: \n",
    "            swap_win += 1 \n",
    "        else: \n",
    "            no_swap_win +=1  \n",
    " \n",
    "print(f'After {trial} trials: Win on Swap {swap_win} - Win when Not swap {no_swap_win}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice((True, False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\" Monte Carlo \"\"\"\n",
    "\n",
    "\"\"\" Epsilon Greedy \"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# all states\n",
    "N_STATES = 19\n",
    "\n",
    "# all states but terminal states\n",
    "STATES = np.arange(1, N_STATES + 1)\n",
    "\n",
    "# start from the middle state\n",
    "START_STATE = 10\n",
    "\n",
    "# two terminal states\n",
    "# an action leading to the left terminal state has reward -1\n",
    "# an action leading to the right terminal state has reward 1\n",
    "END_STATES = [0, N_STATES + 1]\n",
    "\n",
    "# true state values from Bellman equation\n",
    "TRUE_VALUE = np.arange(-20, 22, 2) / 20.0\n",
    "TRUE_VALUE[0] = TRUE_VALUE[N_STATES + 1] = 0.0\n",
    "\n",
    "class ValueFunction:\n",
    "    # @rate: lambda, as it's a keyword in python, so I call it rate\n",
    "    # @stepSize: alpha, step size for update\n",
    "    def __init__(self, rate, step_size, V):\n",
    "        self.rate = rate\n",
    "        self.step_size = step_size\n",
    "        self.weights = V# np.zeros(N_STATES + 2)\n",
    "\n",
    "    # the state value is just the weight\n",
    "    def value(self, state):\n",
    "        return self.weights[state]\n",
    "\n",
    "    # feed the algorithm with new observation\n",
    "    # derived class should override this function\n",
    "    def learn(self, state, reward):\n",
    "        return\n",
    "\n",
    "    # initialize some variables at the beginning of each episode\n",
    "    # must be called at the very beginning of each episode\n",
    "    # derived class should override this function\n",
    "    def new_episode(self):\n",
    "        return\n",
    "\n",
    "class TrueOnlineTemporalDifferenceLambda(ValueFunction):\n",
    "    def __init__(self, rate, step_size, old_state_value, last_state, s, V):\n",
    "        ValueFunction.__init__(self, rate, step_size, V)\n",
    "        # initialize the eligibility trace\n",
    "        self.eligibility = np.zeros(s)\n",
    "        # initialize the beginning state\n",
    "        self.last_state = last_state\n",
    "        # initialize the old state value\n",
    "        self.old_state_value = 0.0\n",
    "\n",
    "    def new_episode(self):\n",
    "        # initialize the eligibility trace\n",
    "        self.eligibility = np.zeros(N_STATES + 2)\n",
    "        # initialize the beginning state\n",
    "        self.last_state = START_STATE\n",
    "        # initialize the old state value\n",
    "        self.old_state_value = 0.0\n",
    "\n",
    "    def learn(self, state, reward):\n",
    "        # update the eligibility trace and weights\n",
    "        last_state_value = self.value(self.last_state)\n",
    "        state_value = self.value(state)\n",
    "        dutch = 1 - self.step_size * self.rate * self.eligibility[self.last_state]\n",
    "        self.eligibility *= self.rate\n",
    "        self.eligibility[self.last_state] += dutch\n",
    "        delta = reward + state_value - last_state_value\n",
    "        self.weights += self.step_size * (delta + last_state_value - self.old_state_value) * self.eligibility\n",
    "        self.weights[self.last_state] -= self.step_size * (last_state_value - self.old_state_value)\n",
    "        self.old_state_value = state_value\n",
    "        self.last_state = state\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\"\"\" TD(λ) \"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def td_lambtha(env, V, policy, lambtha=1, episodes=5000,\n",
    "               max_steps=100, alpha=0.1, gamma=0.99):\n",
    "    \"\"\"\n",
    "    ************************************************************\n",
    "    ****************performs the TD(λ) algorithm****************\n",
    "    ************************************************************\n",
    "    @env: is the openAI environment instance\n",
    "    @V: is a numpy.ndarray of shape (s,) containing the value estimate\n",
    "    @policy: is a function that takes in a state and returns the next\n",
    "             action to take\n",
    "    @lambtha: is the eligibility trace factor\n",
    "    @episodes: is the total number of episodes to train over\n",
    "    @max_steps: is the maximum number of steps per episode\n",
    "    @alpha: is the learning rate\n",
    "    @gamma: is the discount rate\n",
    "    Returns: V, the updated value estimate\n",
    "    \"\"\"\n",
    "    for ep in range(episodes):\n",
    "        # Reseting the environment\n",
    "        state = env.reset()\n",
    "        episode = []\n",
    "        for step in range(max_steps):\n",
    "            # Taking action\n",
    "            action = policy(state)\n",
    "            # Getting the reward and outcome state\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            # Appending results for each state of episode\n",
    "            episode.append([state, action, reward, new_state])\n",
    "            if done:\n",
    "                break\n",
    "            # Incrementing the satete\n",
    "            state = new_state\n",
    "        # Cast and turn episode list to np.ndarray\n",
    "        episode = np.array(episode, dtype=int)\n",
    "        # initiate needed variabes\n",
    "        T = len(episode)  # total number of states starting from 0\n",
    "        G = 0  # empirical return\n",
    "        n = 1  # number of steps\n",
    "        for t in range(T):\n",
    "            state, action, Returns, new_state = episode[t]\n",
    "            # calculate Gt de n step\n",
    "            G = gamma**t * G + Returns  # summing returns (rewards)\n",
    "            Gtn = G + gamma**(n) * V[new_state]  # adding V(s of t+1)\n",
    "            # calculate Gtn lambda by weights decay:\n",
    "            #                             a factor λ with n,  λ^(n−1)\n",
    "            Gtnlamda = (1 - lambtha) * (Gtn + lambtha**(n - 1))  # λ-return\n",
    "            # Value Estimation\n",
    "            if state not in episode[:ep, 0]:\n",
    "                V[state] = (1 - alpha) * (V[state] + Gtnlamda)\n",
    "                V[state] = V[state] + alpha * (Gtnlamda - V[state])\n",
    "                V[state] = V[state] + alpha * (Returns + gamma\n",
    "                                               * V[new_state] - V[state])\n",
    "            n += 1\n",
    "    # Returning the updated Value Estimate\n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.9137  0.9509  0.8348  0.8438  0.6991  0.875   0.6712  0.7066]\n",
      " [ 0.5792  0.7919  0.7782  0.6965  0.7106  0.8752  0.9143  1.    ]\n",
      " [ 0.7902  0.6431  0.8612 -1.      0.7434  0.9372  0.9369  1.    ]\n",
      " [ 0.6604  0.5058  0.6626  0.6149  0.7395 -1.      0.8198  0.7641]\n",
      " [ 0.7059  0.393   0.4348 -1.      0.9327  1.      0.7935  1.    ]\n",
      " [ 1.     -1.     -1.      1.      1.      1.     -1.      1.    ]\n",
      " [ 1.     -1.      1.      1.     -1.      1.     -1.      1.    ]\n",
      " [ 1.      1.      1.     -1.      1.      1.      1.      1.    ]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "td_lambtha = __import__('1-td_lambtha').td_lambtha\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "env = gym.make('FrozenLake8x8-v0')\n",
    "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
    "\n",
    "def policy(s):\n",
    "    p = np.random.uniform()\n",
    "    if p > 0.5:\n",
    "        if s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
    "            return RIGHT\n",
    "        elif s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
    "            return DOWN\n",
    "        elif s // 8 != 0 and env.desc[s // 8 - 1, s % 8] != b'H':\n",
    "            return UP\n",
    "        else:\n",
    "            return LEFT\n",
    "    else:\n",
    "        if s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
    "            return DOWN\n",
    "        elif s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
    "            return RIGHT\n",
    "        elif s % 8 != 0 and env.desc[s // 8, s % 8 - 1] != b'H':\n",
    "            return LEFT\n",
    "        else:\n",
    "            return UP\n",
    "\n",
    "V = np.where(env.desc == b'H', -1, 1).reshape(64).astype('float64') \n",
    "np.set_printoptions(precision=4)\n",
    "print(td_lambtha(env, V, policy, 0.8, episodes=500, max_steps=25, alpha=0.07, gamma=0.95).reshape((8, 8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.8603  0.9059  1.      1.      1.      1.      1.      1.    ]\n",
      " [ 0.6211  0.777   0.8545  0.9174  0.9172  0.917   1.      1.    ]\n",
      " [ 0.5552  0.5735  0.4333 -1.      1.      0.6374  0.9079  1.    ]\n",
      " [ 0.6758  0.6307  0.641   0.3649  0.8372 -1.      1.      1.    ]\n",
      " [ 0.6888  0.7199  0.2744 -1.      0.9198  0.7056  1.      1.    ]\n",
      " [ 0.4074 -1.     -1.      1.      1.      1.     -1.      1.    ]\n",
      " [ 0.9194 -1.      1.      1.     -1.      1.     -1.      1.    ]\n",
      " [ 0.7191  0.705   1.     -1.      1.      1.      1.      1.    ]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "#td_lambtha = __import__('1-td_lambtha').td_lambtha\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "env = gym.make('FrozenLake8x8-v0')\n",
    "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
    "\n",
    "def policy(s):\n",
    "    p = np.random.uniform()\n",
    "    if p > 0.5:\n",
    "        if s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
    "            return RIGHT\n",
    "        elif s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
    "            return DOWN\n",
    "        elif s // 8 != 0 and env.desc[s // 8 - 1, s % 8] != b'H':\n",
    "            return UP\n",
    "        else:\n",
    "            return LEFT\n",
    "    else:\n",
    "        if s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
    "            return DOWN\n",
    "        elif s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
    "            return RIGHT\n",
    "        elif s % 8 != 0 and env.desc[s // 8, s % 8 - 1] != b'H':\n",
    "            return LEFT\n",
    "        else:\n",
    "            return UP\n",
    "\n",
    "V = np.where(env.desc == b'H', -1, 1).reshape(64).astype('float64') \n",
    "np.set_printoptions(precision=4)\n",
    "#env.seed(123)\n",
    "print(td_lambtha(env, V, policy, 0.9).reshape((8, 8)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_lambtha(env, V, policy, lambtha, episodes=5000,\n",
    "               max_steps=100, alpha=0.1, gamma=0.99):\n",
    "    \"\"\"\n",
    "    ***********************************************************\n",
    "    *************perform the Monte Carlo algorithm*************\n",
    "    ***********************************************************\n",
    "    @env: is the openAI environment instance\n",
    "    @V: is a numpy.ndarray of shape (s,) containing the value estimate\n",
    "    @policy: is a function that takes in a state and returns the next action to take\n",
    "    @lambtha: is the eligibility trace factor\n",
    "    @episodes: is the total number of episodes to train over\n",
    "    @max_steps: is the maximum number of steps per episode\n",
    "    @alpha: is the learning rate\n",
    "    @gamma: is the discount rate\n",
    "    Returns: V, the updated value estimate\n",
    "    \"\"\"\n",
    "    s = V.shape[0]\n",
    "    \n",
    "    for ep in range(episodes):\n",
    "        # Reseting the environment each time as per requirement\n",
    "        state = env.reset()\n",
    "        episode = []\n",
    "        R = 0\n",
    "        t = 0\n",
    "        z = 0\n",
    "        states = []\n",
    "        eligibility = [0] * s\n",
    "        for step in range(max_steps):\n",
    "            action = policy(state)\n",
    "            # Taking the action and getting the reward and outcome state\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            eligibility[state] += 1\n",
    "            target = reward + gamma * V[new_state] - V[state]\n",
    "            eligibility[state]  *= gamma * lambtha\n",
    "            V[state] = alpha * target * eligibility[state] \n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            state = new_state\n",
    "         \n",
    "    return V.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(Q, state, epsilon):\n",
    "    \"\"\"\n",
    "    ******************************************************\n",
    "    ***********uses epsilon-greedy to determine***********\n",
    "    *******************the next action********************\n",
    "    ******************************************************\n",
    "    @Q: is a numpy.ndarray containing the q-table\n",
    "    @state: is the current state\n",
    "    @epsilon: is the epsilon to use for the calculation\n",
    "    *** You should sample p with numpy.random.uniformn to determine\n",
    "        if your algorithm should explore or exploit\n",
    "    *** If exploring, you should pick the next action with\n",
    "        numpy.random.randint from all possible actions\n",
    "    Returns:\n",
    "            the next action index\n",
    "    \"\"\"\n",
    "    p = np.random.uniform()\n",
    "    if p < epsilon:\n",
    "        index = np.random.randint(Q.shape[1])\n",
    "    else:\n",
    "        index = np.argmax(Q[state])\n",
    "\n",
    "    return index\n",
    "\n",
    "\n",
    "def sarsa_lambtha(env, Q, lambtha, episodes=5000, max_steps=100,\n",
    "                  alpha=0.1, gamma=0.99, epsilon=1, min_epsilon=0.1, epsilon_decay=0.05):\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        action = epsilon_greedy(Q, state, epsilon=epsilon)\n",
    "        for step in range(max_steps):\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            new_action = epsilon_greedy(Q, new_state, epsilon=epsilon)\n",
    "            Q[state, action] = (((1-lambtha )* Q[state, action])\n",
    "                                + alpha * (reward + gamma * Q[new_state, new_action]\n",
    "                                           - Q[state, action]))\n",
    "        if done:\n",
    "            break\n",
    "        state = new_state\n",
    "        action = new_action\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.069  0.7152 0.0575 0.0883]\n",
      " [0.4237 0.6459 0.4376 0.8918]\n",
      " [0.9637 0.3834 0.7917 0.5289]\n",
      " [0.568  0.9256 0.071  0.0871]\n",
      " [0.0202 0.8326 0.7782 0.87  ]\n",
      " [0.9786 0.7992 0.4615 0.7805]\n",
      " [0.1183 0.6399 0.1434 0.9447]\n",
      " [0.5218 0.4147 0.2646 0.7742]\n",
      " [0.4562 0.5684 0.0188 0.6176]\n",
      " [0.6121 0.6169 0.9437 0.6818]\n",
      " [0.3595 0.437  0.6976 0.0602]\n",
      " [0.6668 0.6706 0.2104 0.1289]\n",
      " [0.3154 0.3637 0.5702 0.4386]\n",
      " [0.9884 0.102  0.2089 0.1613]\n",
      " [0.6531 0.2533 0.4663 0.2444]\n",
      " [0.159  0.1104 0.6563 0.1382]\n",
      " [0.1966 0.3687 0.821  0.0971]\n",
      " [0.8379 0.0961 0.9765 0.4687]\n",
      " [0.9768 0.6048 0.7393 0.0392]\n",
      " [0.2828 0.1202 0.2961 0.1187]\n",
      " [0.318  0.4143 0.0641 0.6925]\n",
      " [0.5666 0.2654 0.5232 0.0939]\n",
      " [0.5759 0.9293 0.3186 0.6674]\n",
      " [0.1318 0.7163 0.2894 0.1832]\n",
      " [0.5865 0.0201 0.8289 0.0047]\n",
      " [0.6778 0.27   0.7352 0.9622]\n",
      " [0.2488 0.5762 0.592  0.5723]\n",
      " [0.2231 0.9527 0.4471 0.8464]\n",
      " [0.6995 0.2974 0.8138 0.3965]\n",
      " [0.8811 0.5813 0.8817 0.6925]\n",
      " [0.7253 0.5013 0.9561 0.644 ]\n",
      " [0.4239 0.6064 0.0192 0.3016]\n",
      " [0.6602 0.2901 0.618  0.4288]\n",
      " [0.1355 0.2983 0.57   0.5909]\n",
      " [0.5743 0.6532 0.6521 0.4314]\n",
      " [0.8965 0.3676 0.4359 0.8919]\n",
      " [0.8062 0.7039 0.1002 0.9195]\n",
      " [0.7142 0.9988 0.1494 0.8681]\n",
      " [0.1625 0.6156 0.1238 0.848 ]\n",
      " [0.8073 0.5691 0.4072 0.0692]\n",
      " [0.6974 0.4535 0.7221 0.8664]\n",
      " [0.9755 0.8558 0.0117 0.36  ]\n",
      " [0.73   0.1716 0.521  0.0543]\n",
      " [0.2    0.0185 0.7937 0.2239]\n",
      " [0.3454 0.9281 0.7044 0.0318]\n",
      " [0.1647 0.6215 0.5772 0.2379]\n",
      " [0.9342 0.614  0.5356 0.5899]\n",
      " [0.7301 0.3119 0.3982 0.2098]\n",
      " [0.1862 0.9444 0.7396 0.4905]\n",
      " [0.2274 0.2544 0.058  0.4344]\n",
      " [0.3118 0.6963 0.3778 0.1796]\n",
      " [0.0247 0.0672 0.6794 0.4537]\n",
      " [0.5366 0.8967 0.9903 0.2169]\n",
      " [0.6631 0.2633 0.0207 0.7584]\n",
      " [0.32   0.3835 0.5883 0.831 ]\n",
      " [0.629  0.8727 0.2735 0.798 ]\n",
      " [0.1856 0.9528 0.6875 0.2155]\n",
      " [0.9474 0.7309 0.2539 0.2133]\n",
      " [0.5182 0.0257 0.2075 0.4247]\n",
      " [0.3742 0.4636 0.2776 0.5868]\n",
      " [0.8639 0.1175 0.5174 0.1321]\n",
      " [0.7169 0.3961 0.5654 0.1833]\n",
      " [0.1448 0.4881 0.3556 0.9404]\n",
      " [0.7653 0.7487 0.9037 0.0834]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "#sarsa_lambtha = __import__('2-sarsa_lambtha').sarsa_lambtha\n",
    "\n",
    "np.random.seed(0)\n",
    "env = gym.make('FrozenLake8x8-v0')\n",
    "Q = np.random.uniform(size=(64, 4))\n",
    "np.set_printoptions(precision=4)\n",
    "print(sarsa_lambtha(env, Q, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
