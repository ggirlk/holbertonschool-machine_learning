{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!touch 0-l2_reg_cost.py\n",
    "!chmod +x *.py\n",
    "!#cp 6-dropout_create_layer.py 7-early_stopping.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.41842822]\n",
      "[0.45158952]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "#l2_reg_cost = __import__('0-l2_reg_cost').l2_reg_cost np.linalg.norm\n",
    "def l2_reg_cost(cost, lambtha, weights, L, m):\n",
    "    \"\"\" doc \"\"\"\n",
    "    sw = 0\n",
    "    for i in range(1, L+1):\n",
    "        sw += np.linalg.norm(weights['W'+str(i)], ord='fro')\n",
    "    l2 = (lambtha*sw)/(2*m)\n",
    "    return (cost+l2)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(0)\n",
    "\n",
    "    weights = {}\n",
    "    weights['W1'] = np.random.randn(256, 784)\n",
    "    weights['W2'] = np.random.randn(128, 256)\n",
    "    weights['W3'] = np.random.randn(10, 128)\n",
    "\n",
    "    cost = np.abs(np.random.randn(1))\n",
    "\n",
    "    print(cost)\n",
    "    cost = l2_reg_cost(cost, 0.1, weights, 3, 1000)\n",
    "    print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.76405235  0.40015721  0.97873798 ...  0.52130375  0.61192719\n",
      "  -1.34149673]\n",
      " [ 0.47689837  0.14844958  0.52904524 ...  0.0960042  -0.0451133\n",
      "   0.07912172]\n",
      " [ 0.85053068 -0.83912419 -1.01177408 ... -0.07223876  0.31112445\n",
      "  -1.07836109]\n",
      " ...\n",
      " [-0.60467085  0.54751161 -1.23317415 ...  0.82895532  1.44161136\n",
      "   0.18972404]\n",
      " [-0.41044606  0.85719512  0.71789835 ... -0.73954771  0.5074628\n",
      "   1.23022874]\n",
      " [ 0.43129249  0.60767018 -0.07749988 ... -0.26611561  2.52287972\n",
      "   0.73131543]]\n",
      "*****************\n",
      "[[ 1.76405199  0.40015713  0.97873779 ...  0.52130364  0.61192707\n",
      "  -1.34149646]\n",
      " [ 0.47689827  0.14844955  0.52904513 ...  0.09600419 -0.04511329\n",
      "   0.07912171]\n",
      " [ 0.85053051 -0.83912402 -1.01177388 ... -0.07223874  0.31112438\n",
      "  -1.07836088]\n",
      " ...\n",
      " [-0.60467073  0.5475115  -1.2331739  ...  0.82895516  1.44161107\n",
      "   0.189724  ]\n",
      " [-0.41044598  0.85719495  0.71789821 ... -0.73954756  0.5074627\n",
      "   1.2302285 ]\n",
      " [ 0.4312924   0.60767006 -0.07749987 ... -0.26611556  2.52287922\n",
      "   0.73131529]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "#l2_reg_gradient_descent = __import__('1-l2_reg_gradient_descent').l2_reg_gradient_descent\n",
    "\n",
    "def l2_reg_gradient_descent(Y, weights, cache, alpha, lambtha, L):\n",
    "    \"\"\" doc \"\"\"\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    def dw(dz, x):\n",
    "        \"\"\" weight derivative \"\"\"\n",
    "        return np.matmul(dz, x.T)/m\n",
    "\n",
    "    def db(dz):\n",
    "        \"\"\" bias derivative\"\"\"\n",
    "        return np.mean(dz, axis=1, keepdims=True)\n",
    "\n",
    "    def der(x):\n",
    "        \"\"\" tanh derivative \"\"\"\n",
    "        return 1 - (x**2)\n",
    "\n",
    "    def dz(wi, dzi, gprimei):\n",
    "        \"\"\" z derivative \"\"\"\n",
    "        x = np.matmul(wi.T, dzi)\n",
    "        return np.multiply(gprimei, x)\n",
    "\n",
    "    n = L\n",
    "    wb = weights.copy()\n",
    "    dzi = np.subtract(cache['A'+str(n)], Y)\n",
    "    for i in reversed(range(1, n+1)):\n",
    "        Ai = cache['A'+str(i)]\n",
    "        Ai_1 = cache['A'+str(i-1)]\n",
    "        b = wb['b'+str(i)]\n",
    "        if i == n:\n",
    "            dzi = np.subtract(cache['A'+str(n)], Y)\n",
    "        else:\n",
    "            w = wb['W'+str(i+1)]\n",
    "            dzi = dz(w, dzi, der(Ai))\n",
    "        dwi = dw(dzi, Ai_1)\n",
    "        dbi = db(dzi)\n",
    "        weights['b'+str(i)] = wb['b'+str(i)]*(1-alpha*lambtha/m) - dbi\n",
    "        weights['W'+str(i)] = wb['W'+str(i)]*(1-alpha*lambtha/m) - dwi\n",
    "        \"\"\"weights['b'+str(i)] = wb['b'+str(i)]-alpha*(dbi + lambtha/m*wb['b'+str(i)])\n",
    "        weights['W'+str(i)] = wb['W'+str(i)]-alpha*(dwi + lambtha/m*wb['W'+str(i)])\"\"\"\n",
    "\n",
    "def one_hot(Y, classes):\n",
    "    \"\"\"convert an array to a one-hot matrix\"\"\"\n",
    "    m = Y.shape[0]\n",
    "    one_hot = np.zeros((classes, m))\n",
    "    one_hot[Y, np.arange(m)] = 1\n",
    "    return one_hot\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lib= np.load('../data/MNIST.npz')\n",
    "    X_train_3D = lib['X_train']\n",
    "    Y_train = lib['Y_train']\n",
    "    X_train = X_train_3D.reshape((X_train_3D.shape[0], -1)).T\n",
    "    Y_train_oh = one_hot(Y_train, 10)\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    weights = {}\n",
    "    weights['W1'] = np.random.randn(256, 784)\n",
    "    weights['b1'] = np.zeros((256, 1))\n",
    "    weights['W2'] = np.random.randn(128, 256)\n",
    "    weights['b2'] = np.zeros((128, 1))\n",
    "    weights['W3'] = np.random.randn(10, 128)\n",
    "    weights['b3'] = np.zeros((10, 1))\n",
    "\n",
    "    cache = {}\n",
    "    cache['A0'] = X_train\n",
    "    cache['A1'] = np.tanh(np.matmul(weights['W1'], cache['A0']) + weights['b1'])\n",
    "    cache['A2'] = np.tanh(np.matmul(weights['W2'], cache['A1']) + weights['b2'])\n",
    "    Z3 = np.matmul(weights['W3'], cache['A2']) + weights['b3']\n",
    "    cache['A3'] = np.exp(Z3) / np.sum(np.exp(Z3), axis=0)\n",
    "    print(weights['W1'])\n",
    "    print(\"*****************\")\n",
    "    l2_reg_gradient_descent(Y_train_oh, weights, cache, 0.1, 0.1, 3)\n",
    "    print(weights['W1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57.08354   50.44892    6.2935305 56.925423  49.81497    6.32806\n",
      " 56.863335  50.60656    6.2734222 56.959087  50.246857   6.2164383\n",
      " 57.05261   50.26753    6.336273  56.80382   50.540554   6.2989316\n",
      " 56.91663   49.99504    6.215603  57.192356  50.322174   6.265082\n",
      " 56.718937  50.35683    6.4219365 57.047997  50.330017   6.2452383\n",
      " 56.95557   50.436375   6.4484167 57.039894  50.109703   6.4311247\n",
      " 56.980392  50.421574   6.404589  57.18429   50.350395   6.3037987\n",
      " 56.999306  50.17823    6.3784227 57.21298   50.517464   6.295838\n",
      " 57.168133  50.276833   6.1733427 57.100937  50.507557   6.3956137\n",
      " 56.579865  50.689434   6.3352137 56.985645  50.21796    6.4832354\n",
      " 56.978416  50.368896   6.421719  57.081974  50.03368    6.331265\n",
      " 56.929684  50.10806    6.339952  56.82241   50.149315   6.370785\n",
      " 57.13528   50.05122    6.379484  56.971165  50.455505   6.251236\n",
      " 57.09689   50.299023   6.2998137 56.964302  50.040157   6.248044\n",
      " 56.958668  50.429115   6.2822537 57.196697  49.807766   6.5139313\n",
      " 56.863186  50.389862   6.3247833 56.9468    50.182117   6.2291746\n",
      " 57.36236   50.359905   6.365651  56.997726  50.163013   6.3564496\n",
      " 57.132053  50.55742    6.238514  56.90875   50.250908   6.306306\n",
      " 57.133087  50.29403    6.3378267 56.978664  50.341095   6.5080767\n",
      " 56.749004  50.563316   6.2057743 56.87513   50.441772   6.3358183\n",
      " 56.972347  50.314342   6.2660213 57.01943   50.375153   6.412669\n",
      " 57.20394   50.222164   6.251912  56.896023  49.96297    6.3129034\n",
      " 56.78005   50.20601    6.274102  57.276974  50.211136   6.3383164\n",
      " 57.20743   50.00677    6.367693  57.164135  50.278854   6.3055086\n",
      " 56.76028   50.28922    6.3367558 56.963192  50.356625   6.3734846\n",
      " 57.06763   50.16634    6.290949  56.743824  50.422356   6.2465725\n",
      " 57.237022  50.364384   6.222228  56.976086  50.53397    6.3817873\n",
      " 56.82274   50.243156   6.306503  57.127254  50.212986   6.329155\n",
      " 57.019375  50.07198    6.30231   56.98652   50.180634   6.346157\n",
      " 56.918816  50.445927   6.2769437 56.870083  50.143017   6.3815217\n",
      " 56.91791   50.30129    6.1958323 56.90802   50.484467   6.3133574\n",
      " 57.116528  50.57428    6.1589823 56.92338   50.654465   6.2472763\n",
      " 57.15807   50.213818   6.3031893 56.83812   50.522007   6.4673657\n",
      " 56.931805  50.27328    6.312047  57.167168  50.50618    6.4073005\n",
      " 57.142773  50.168495   6.446484  56.91315   50.47375    6.2952013]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#l2_reg_cost = __import__('2-l2_reg_cost').l2_reg_cost\n",
    "def l2_reg_cost(cost):\n",
    "    \"\"\" doc \"\"\"\n",
    "\n",
    "    return cost+tf.losses.get_regularization_losses()\n",
    "\n",
    "def one_hot(Y, classes):\n",
    "    \"\"\"convert an array to a one-hot matrix\"\"\"\n",
    "    m = Y.shape[0]\n",
    "    oh = np.zeros((classes, m))\n",
    "    oh[Y, np.arange(m)] = 1\n",
    "    return oh\n",
    "\n",
    "np.random.seed(4)\n",
    "m = np.random.randint(1000, 2000)\n",
    "c = 10\n",
    "lib= np.load('../data/MNIST.npz')\n",
    "\n",
    "X = lib['X_train'][:m].reshape((m, -1))\n",
    "Y = one_hot(lib['Y_train'][:m], c).T\n",
    "\n",
    "n0 = X.shape[1]\n",
    "n1, n2 = np.random.randint(10, 1000, 2)\n",
    "\n",
    "lam = np.random.uniform(0.01)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, n0))\n",
    "y = tf.placeholder(tf.float32, (None, c))\n",
    "\n",
    "a1 = tf.layers.Dense(n1, activation=tf.nn.tanh, kernel_initializer=tf.contrib.layers.variance_scaling_initializer(mode=\"FAN_AVG\"), kernel_regularizer=tf.contrib.layers.l2_regularizer(lam))(x)\n",
    "a2 = tf.layers.Dense(n2, activation=tf.nn.sigmoid, kernel_initializer=tf.contrib.layers.variance_scaling_initializer(mode=\"FAN_AVG\"), kernel_regularizer=tf.contrib.layers.l2_regularizer(lam))(a1)\n",
    "y_pred = tf.layers.Dense(c, activation=None, kernel_initializer=tf.contrib.layers.variance_scaling_initializer(mode=\"FAN_AVG\"), kernel_regularizer=tf.contrib.layers.l2_regularizer(lam))(a2)\n",
    "\n",
    "cost = tf.losses.softmax_cross_entropy(y, y_pred)\n",
    "l2_cost = l2_reg_cost(cost)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(l2_cost, feed_dict={x: X, y: Y}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Fetch argument None has invalid type <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-232-cb8d29fb5139>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml2_cost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mY_train_oh\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1137\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    469\u001b[0m     \"\"\"\n\u001b[1;32m    470\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m       raise TypeError('Fetch argument %r has invalid type %r' % (fetch,\n\u001b[0;32m--> 258\u001b[0;31m                                                                  type(fetch)))\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Fetch argument None has invalid type <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "l2_reg_cost = __import__('2-l2_reg_cost').l2_reg_cost\n",
    "#l2_reg_create_layer = __import__('3-l2_reg_create_layer').l2_reg_create_layer\n",
    "def l2_reg_create_layer(prev, n, activation, lambtha):\n",
    "    \"\"\" doc \"\"\"\n",
    "    init = (tf.contrib.layers.\n",
    "            variance_scaling_initializer(mode=\"FAN_AVG\"))\n",
    "    freg = tf.contrib.layers.l2_regularizer(lambtha)\n",
    "    layer = tf.layers.Dense(n,activation, name='layer',\n",
    "                            kernel_initializer=init,\n",
    "                            kernel_regularizer=freg)(prev)\n",
    "    return (layer)\n",
    "def one_hot(Y, classes):\n",
    "    \"\"\"convert an array to a one-hot matrix\"\"\"\n",
    "    m = Y.shape[0]\n",
    "    one_hot = np.zeros((m, classes))\n",
    "    one_hot[np.arange(m), Y] = 1\n",
    "    return one_hot\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lib= np.load('../data/MNIST.npz')\n",
    "    X_train_3D = lib['X_train']\n",
    "    Y_train = lib['Y_train']\n",
    "    X_train = X_train_3D.reshape((X_train_3D.shape[0], -1))\n",
    "    Y_train_oh = one_hot(Y_train, 10)\n",
    "\n",
    "    tf.set_random_seed(0)\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "    h1 = l2_reg_create_layer(x, 256, tf.nn.tanh, 0.1)\n",
    "    y_pred = l2_reg_create_layer(x, 10, None, 0.)\n",
    "    cost = tf.losses.softmax_cross_entropy(y, y_pred)\n",
    "    l2_cost = l2_reg_cost(cost)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(sess.run(l2_cost, feed_dict={x: X_train, y: Y_train_oh}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A0 [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "A1 [[-1.24999999 -1.25       -1.24999945 ... -1.25       -1.25\n",
      "  -1.25      ]\n",
      " [ 1.25        1.24999777  1.25       ...  0.37738875  1.24999717\n",
      "  -1.24999889]\n",
      " [ 0.19383179 -0.80653094 -1.24950714 ...  1.24253535  1.08653948\n",
      "  -1.20190135]\n",
      " ...\n",
      " [-1.25       -1.25        0.         ... -0.         -1.25\n",
      "  -1.24999852]\n",
      " [-1.0858595  -1.25        0.         ...  1.24972487 -0.88878698\n",
      "  -1.24999933]\n",
      " [ 1.25        1.24999648  0.2057473  ...  0.          1.23194191\n",
      "  -1.24908257]]\n",
      "A2 [[-1.25        0.          1.24985922 ... -1.25        0.\n",
      "   1.24996854]\n",
      " [-0.         -0.         -0.         ... -1.24996232 -0.70684864\n",
      "   1.25      ]\n",
      " [-1.25        0.          0.18486152 ... -1.24999999 -1.25\n",
      "  -1.24999989]\n",
      " ...\n",
      " [ 1.2404131   1.25        1.25       ...  1.1670038   1.25\n",
      "  -0.        ]\n",
      " [ 1.25        1.25       -1.24998041 ...  1.2400913  -1.25\n",
      "   1.23620006]\n",
      " [ 0.93426582  1.25        1.25       ...  1.24999867 -1.25\n",
      "  -0.        ]]\n",
      "A3 [[9.13222086e-07 1.53352996e-09 4.02988574e-13 ... 2.93685964e-04\n",
      "  2.21615443e-11 7.95945899e-04]\n",
      " [4.10709405e-16 4.27810333e-11 7.38725096e-07 ... 2.05423847e-17\n",
      "  2.66482686e-09 1.74341031e-12]\n",
      " [9.82953561e-01 9.88655425e-01 9.73580864e-01 ... 1.14493065e-03\n",
      "  9.28074126e-10 1.92423905e-13]\n",
      " ...\n",
      " [3.03047424e-04 1.11981605e-02 4.72284535e-05 ... 1.25781567e-20\n",
      "  9.57462819e-01 3.33328605e-13]\n",
      " [3.20689297e-11 7.42324257e-08 5.62529910e-19 ... 2.05682936e-16\n",
      "  1.07622653e-12 1.41200115e-02]\n",
      " [5.06603174e-06 8.50852457e-11 5.51467429e-10 ... 9.98493133e-01\n",
      "  1.97896353e-14 2.38078250e-05]]\n",
      "D1 [[1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 1]\n",
      " ...\n",
      " [1 1 0 ... 0 1 1]\n",
      " [1 1 0 ... 1 1 1]\n",
      " [1 1 1 ... 0 1 1]]\n",
      "D2 [[1 0 1 ... 1 0 1]\n",
      " [0 0 0 ... 1 1 1]\n",
      " [1 0 1 ... 1 1 1]\n",
      " ...\n",
      " [1 1 1 ... 1 1 0]\n",
      " [1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "#dropout_forward_prop = __import__('4-dropout_forward_prop').dropout_forward_prop\n",
    "\n",
    "\n",
    "def dropout_forward_prop(X, weights, L, keep_prob):\n",
    "    \"\"\" doc \"\"\"\n",
    "    def softmax(z):\n",
    "        \"\"\" softmax function \"\"\"\n",
    "        return np.exp(z)/(np.sum(np.exp(z), axis=0))\n",
    "    def tanh(z):\n",
    "        \"\"\" tanh function \"\"\"\n",
    "        return np.tanh(z)\n",
    "    cache = {}\n",
    "    cache[\"A0\"] = X\n",
    "    for i in range(1, L+1):\n",
    "        W = weights[\"W\"+str(i)]\n",
    "        b = weights[\"b\"+str(i)]\n",
    "        A = cache['A'+str(i-1)]\n",
    "        Z = np.dot(W, A) + b\n",
    "        if i != L:\n",
    "            cache['A'+str(i)] = tanh(Z)\n",
    "            A = cache['A'+str(i)]\n",
    "            cache[\"D\"+str(i)] = np.random.rand(A.shape[0], A.shape[1]) < keep_prob\n",
    "            cache[\"D\"+str(i)] = np.where(cache[\"D\"+str(i)] < keep_prob, 0, 1)\n",
    "            cache['A'+str(i)] = np.multiply(A, cache[\"D\"+str(i)]) / keep_prob\n",
    "        else:\n",
    "            cache['A'+str(i)] = softmax(Z)\n",
    "    return cache\n",
    "\"\"\"\n",
    "cache[\"D\"+str(i)] = np.random.randn(A.shape[1], A.shape[0])\n",
    "cache[\"D\"+str(i)] = np.where(cache[\"D\"+str(i)] < keep_prob, 1, 0)\n",
    "W = tf.get_variable(\"W\",shape=[512,128],initializer=init)\n",
    "b = tf.get_variable(\"b\",initializer=tf.zeros([128]))\n",
    "\n",
    "dropped = tf.nn.dropout(prev_layer,keep_prob=current_keep_prob)\n",
    "\n",
    "dense = tf.matmul(dropped,W)+b\n",
    "act = tf.nn.relu(dense)\"\"\"\n",
    "\n",
    "def one_hot(Y, classes):\n",
    "    \"\"\"convert an array to a one-hot matrix\"\"\"\n",
    "    m = Y.shape[0]\n",
    "    one_hot = np.zeros((classes, m))\n",
    "    one_hot[Y, np.arange(m)] = 1\n",
    "    return one_hot\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lib= np.load('../data/MNIST.npz')\n",
    "    X_train_3D = lib['X_train']\n",
    "    Y_train = lib['Y_train']\n",
    "    X_train = X_train_3D.reshape((X_train_3D.shape[0], -1)).T\n",
    "    Y_train_oh = one_hot(Y_train, 10)\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    weights = {}\n",
    "    weights['W1'] = np.random.randn(256, 784)\n",
    "    weights['b1'] = np.zeros((256, 1))\n",
    "    weights['W2'] = np.random.randn(128, 256)\n",
    "    weights['b2'] = np.zeros((128, 1))\n",
    "    weights['W3'] = np.random.randn(10, 128)\n",
    "    weights['b3'] = np.zeros((10, 1))\n",
    "    keep_prob=0.8\n",
    "\n",
    "    cache = dropout_forward_prop(X_train, weights, 3, 0.8)\n",
    "    for k, v in sorted(cache.items()):\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.9282086  -0.71324613 -1.33191318 ... -2.14202626 -0.07737407\n",
      "   0.99832167]\n",
      " [-0.0237149  -0.18364778  0.08337452 ... -0.06093055 -0.03924408\n",
      "  -2.17625294]\n",
      " [-0.16181888  0.49237435 -0.47196279 ...  0.97504077  0.16272698\n",
      "   0.56159916]\n",
      " ...\n",
      " [ 0.39842474 -0.09870005  1.32173992 ... -0.33210834  0.66215988\n",
      "   0.87211421]\n",
      " [ 0.15767221  0.42236212  1.004765   ...  0.69883284  0.70857088\n",
      "  -0.44427252]\n",
      " [ 2.68588811 -0.60351958 -1.0759598  ... -1.2437044   0.69462324\n",
      "   1.00090403]]\n",
      "[[-1.92044686 -0.71894673 -1.32811693 ... -2.14071955 -0.07158198\n",
      "   0.98206832]\n",
      " [-0.03706116 -0.17088483  0.07798748 ... -0.07245569 -0.0491215\n",
      "  -2.16245276]\n",
      " [-0.17198668  0.49842244 -0.47369328 ...  0.96880194  0.15497217\n",
      "   0.5693131 ]\n",
      " ...\n",
      " [ 0.41997262 -0.11452751  1.32873227 ... -0.31312321  0.67162237\n",
      "   0.85928296]\n",
      " [ 0.13702353  0.44237056  1.00139188 ...  0.68128208  0.69020934\n",
      "  -0.43055442]\n",
      " [ 2.66514017 -0.59204122 -1.08943163 ... -1.26238074  0.69280683\n",
      "   1.02353101]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "#dropout_forward_prop = __import__('4-dropout_forward_prop').dropout_forward_prop\n",
    "#dropout_gradient_descent = __import__('5-dropout_gradient_descent').dropout_gradient_descent\n",
    "\n",
    "def dropout_gradient_descent(Y, weights, cache, alpha, keep_prob, L):\n",
    "    \"\"\" Calculate one pass of gradient descent on the neuron \"\"\"\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    def dw(dz, x):\n",
    "        \"\"\" weight derivative \"\"\"\n",
    "        return np.matmul(dz, x.T)/m\n",
    "\n",
    "    def db(dz):\n",
    "        \"\"\" bias derivative\"\"\"\n",
    "        return np.sum(dz, axis=1, keepdims=True)/m\n",
    "\n",
    "    def der(x):\n",
    "        \"\"\" derivatives tanh activation func \"\"\"\n",
    "        return 1 - (x**2)\n",
    "\n",
    "    def dz(wi, dzi, gprimei, i):\n",
    "        \"\"\" z derivative \"\"\"\n",
    "        x = np.matmul(wi.T, dzi)\n",
    "        dgprime = np.multiply(gprimei, cache[\"D\"+str(i)])/keep_prob\n",
    "        return np.multiply(dgprime, x)\n",
    "\n",
    "    n = L\n",
    "    wb = weights.copy()\n",
    "    dzi = np.subtract(cache['A'+str(n)], Y)\n",
    "    for i in reversed(range(1, n+1)):\n",
    "        Ai = cache['A'+str(i)]\n",
    "        Ai_1 = cache['A'+str(i-1)]\n",
    "        b = wb['b'+str(i)]\n",
    "        if i == n:\n",
    "            dzi = np.subtract(cache['A'+str(n)], Y)\n",
    "        else:\n",
    "            w = wb['W'+str(i+1)]\n",
    "            dzi = dz(w, dzi, der(Ai), i)\n",
    "        dwi = dw(dzi, Ai_1)\n",
    "        dbi = db(dzi)\n",
    "        weights['b'+str(i)] = wb['b'+str(i)]-alpha*dbi\n",
    "        weights['W'+str(i)] = wb['W'+str(i)]-alpha*dwi\n",
    "\n",
    "def one_hot(Y, classes):\n",
    "    \"\"\"convert an array to a one-hot matrix\"\"\"\n",
    "    m = Y.shape[0]\n",
    "    one_hot = np.zeros((classes, m))\n",
    "    one_hot[Y, np.arange(m)] = 1\n",
    "    return one_hot\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lib= np.load('../data/MNIST.npz')\n",
    "    X_train_3D = lib['X_train']\n",
    "    Y_train = lib['Y_train']\n",
    "    X_train = X_train_3D.reshape((X_train_3D.shape[0], -1)).T\n",
    "    Y_train_oh = one_hot(Y_train, 10)\n",
    "\n",
    "    np.random.seed(0)\n",
    "\n",
    "    weights = {}\n",
    "    weights['W1'] = np.random.randn(256, 784)\n",
    "    weights['b1'] = np.zeros((256, 1))\n",
    "    weights['W2'] = np.random.randn(128, 256)\n",
    "    weights['b2'] = np.zeros((128, 1))\n",
    "    weights['W3'] = np.random.randn(10, 128)\n",
    "    weights['b3'] = np.zeros((10, 1))\n",
    "\n",
    "    cache = dropout_forward_prop(X_train, weights, 3, 0.8)\n",
    "    print(weights['W2'])\n",
    "    dropout_gradient_descent(Y_train_oh, weights, cache, 0.1, 0.8, 3)\n",
    "    print(weights['W2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.         -1.         -1.         ...  1.         -1.\n",
      "  -1.        ]\n",
      " [ 1.         -1.          0.99999076 ...  1.         -1.\n",
      "  -1.        ]\n",
      " [-1.         -1.          1.         ...  1.         -1.\n",
      "  -1.        ]\n",
      " ...\n",
      " [-1.         -1.          1.         ...  1.         -1.\n",
      "  -1.        ]\n",
      " [-1.         -1.          1.         ...  1.         -1.\n",
      "  -1.        ]\n",
      " [-1.         -1.         -0.99932134 ...  1.         -1.\n",
      "  -1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#dropout_create_layer = __import__('6-dropout_create_layer').dropout_create_layer\n",
    "\"\"\"def dropout_create_layer(prev, n, activation, keep_prob):\n",
    "    \"\" doc \"\n",
    "    dropped = tf.nn.dropout(prev, keep_prob=keep_prob)\n",
    "    init = (tf.contrib.layers.\n",
    "            variance_scaling_initializer(mode=\"FAN_AVG\"))\n",
    "    layer = tf.layers.Dense(n, activation, name='layer',\n",
    "                            kernel_initializer=init)(dropped)\n",
    "    return layer\"\"\"\n",
    "def dropout_create_layer(prev, n, activation, keep_prob):\n",
    "    \"\"\" doc \"\"\"\n",
    "    dropped = tf.layers.Dropout(rate=keep_prob)\n",
    "    init = (tf.contrib.layers.\n",
    "            variance_scaling_initializer(mode=\"FAN_AVG\"))\n",
    "    layer = tf.layers.Dense(n, activation, name='layer',\n",
    "                            kernel_initializer=init,\n",
    "                            activity_regularizer=dropped)(prev)\n",
    "    return (layer)\n",
    "if __name__ == '__main__':\n",
    "    tf.set_random_seed(0)\n",
    "    np.random.seed(0)\n",
    "    x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "    X = np.random.randint(0, 256, size=(10, 784))\n",
    "    a = dropout_create_layer(x, 256, tf.nn.tanh, 0.8)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(sess.run(a, feed_dict={x: X}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(False, 0)\n",
      "(False, 3)\n",
      "(False, 9)\n",
      "(True, 15)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "#early_stopping = __import__('7-early_stopping').early_stopping\n",
    "def early_stopping(cost, opt_cost, threshold, patience, count):\n",
    "    \"\"\" determines if you should stop gradient descent early \"\"\"\n",
    "    test = opt_cost - cost > threshold\n",
    "    if test:\n",
    "        count = 0\n",
    "    else:\n",
    "        count += 1\n",
    "    return (count >= patience, count)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(early_stopping(1.0, 1.9, 0.5, 15, 5))\n",
    "    print(early_stopping(1.1, 1.5, 0.5, 15, 2))\n",
    "    print(early_stopping(1.0, 1.5, 0.5, 15, 8))\n",
    "    print(early_stopping(1.0, 1.5, 0.5, 15, 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
