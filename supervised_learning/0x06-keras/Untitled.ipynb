{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cp 7-train.py 8-train.py\n",
    "#!touch m.py\n",
    "!chmod +x *.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_197 (Dense)            (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dropout_104 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_198 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_105 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_199 (Dense)            (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[<tf.Tensor 'kernel/Regularizer_149/add:0' shape=() dtype=float32>, <tf.Tensor 'kernel/Regularizer_150/add:0' shape=() dtype=float32>, <tf.Tensor 'kernel/Regularizer_151/add:0' shape=() dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "#build_model = __import__('0-sequential').build_model\n",
    "\"\"\" doc \"\"\"\n",
    "import tensorflow.keras as K\n",
    "\n",
    "\n",
    "def build_model(nx, layers, activations, lambtha, keep_prob):\n",
    "    \"\"\" doc \"\"\"\n",
    "    model = K.Sequential()\n",
    "    for i in range(len(layers)):\n",
    "        init = K.initializers.VarianceScaling(mode=\"fan_avg\")\n",
    "        # freg = K.layers.ActivityRegularization(l2=lambtha)\n",
    "        freg = K.regularizers.l2(lambtha)\n",
    "        layer = K.layers.Dense(layers[i], input_dim=nx,\n",
    "                               activation=activations[i],\n",
    "                               kernel_initializer=init,\n",
    "                               kernel_regularizer=freg)\n",
    "        model.add(layer)\n",
    "        if i != len(layers)-1:\n",
    "            dropped = K.layers.Dropout(rate=1-keep_prob)\n",
    "            model.add(dropped)\n",
    "    return (model)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    network = build_model(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "    network.summary()\n",
    "    print(network.losses)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "[<tf.Tensor 'dense/kernel/Regularizer/add:0' shape=() dtype=float32>, <tf.Tensor 'dense_1/kernel/Regularizer/add:0' shape=() dtype=float32>, <tf.Tensor 'dense_2/kernel/Regularizer/add:0' shape=() dtype=float32>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_103 (InputLayer)       (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_409 (Dense)            (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dropout_309 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_410 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_310 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_411 (Dense)            (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[<tf.Tensor 'kernel/Regularizer_276/add:0' shape=() dtype=float32>, <tf.Tensor 'kernel/Regularizer_277/add:0' shape=() dtype=float32>, <tf.Tensor 'kernel/Regularizer_278/add:0' shape=() dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "def build_model(nx, layers, activations, lambtha, keep_prob):\n",
    "    inputs = K.Input(shape=(nx,))\n",
    "    freg = K.regularizers.l2(lambtha)\n",
    "    x = K.layers.Dense(layers[0], activation=activations[0],\n",
    "                           kernel_regularizer=freg)(inputs)\n",
    "    for i in range(1, len(layers)):\n",
    "        x = K.layers.Dropout(rate=1-keep_prob)(x)\n",
    "        x = K.layers.Dense(layers[i], activation=activations[i],\n",
    "                           kernel_regularizer=freg)(x)\n",
    "        \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return (model)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    network = build_model(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "    network.summary()\n",
    "    print(network.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical_crossentropy\n",
      "['accuracy']\n",
      "<class 'tensorflow.python.keras.optimizers.Adam'>\n",
      "(0.01, 0.99, 0.9)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "build_model = __import__('1-input').build_model\n",
    "# optimize_model = __import__('2-optimize').optimize_model\n",
    "def optimize_model(network, alpha, beta1, beta2):\n",
    "    \"\"\" doc \"\"\"\n",
    "    optimizer = K.optimizers.Adam(alpha, beta1, beta2)\n",
    "    network.compile(optimizer,\n",
    "                    loss=\"categorical_crossentropy\",\n",
    "                    metrics=['accuracy'])\n",
    "    return None\n",
    "if __name__ == '__main__':\n",
    "    model = build_model(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "    optimize_model(model, 0.01, 0.99, 0.9)\n",
    "    print(model.loss)\n",
    "    print(model.metrics)\n",
    "    opt = model.optimizer\n",
    "    print(opt.__class__)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(sess.run((opt.lr, opt.beta_1, opt.beta_2))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1 4]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "#one_hot = __import__('3-one_hot').one_hot\n",
    "def one_hot(labels, classes=None):\n",
    "    \"\"\" doc \"\"\"\n",
    "    return K.utils.to_categorical(labels, classes)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    labels = np.load('../data/MNIST.npz')['Y_train'][:10]\n",
    "    print(labels)\n",
    "    print(one_hot(labels))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 11s 225us/step - loss: 0.3327 - acc: 0.9195\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 0.1763 - acc: 0.9650\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 0.1434 - acc: 0.9750\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 0.1257 - acc: 0.9804\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 0.1167 - acc: 0.9824\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Main file\n",
    "\"\"\"\n",
    "\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.backend.set_session(sess)\n",
    "\n",
    "# Imports\n",
    "build_model = __import__('1-input').build_model\n",
    "optimize_model = __import__('2-optimize').optimize_model\n",
    "one_hot = __import__('3-one_hot').one_hot\n",
    "train_model = __import__('4-train').train_model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_train = datasets['X_train']\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    Y_train = datasets['Y_train']\n",
    "    Y_train_oh = one_hot(Y_train)\n",
    "\n",
    "    lambtha = 0.0001\n",
    "    keep_prob = 0.95\n",
    "    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "    alpha = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    optimize_model(network, alpha, beta1, beta2)\n",
    "    batch_size = 64\n",
    "    epochs = 5\n",
    "    train_model(network, X_train, Y_train_oh, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 13s 253us/step - loss: 0.3306 - acc: 0.9185 - val_loss: 0.1900 - val_acc: 0.9603\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 12s 232us/step - loss: 0.1770 - acc: 0.9657 - val_loss: 0.1579 - val_acc: 0.9698\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 12s 234us/step - loss: 0.1431 - acc: 0.9756 - val_loss: 0.1586 - val_acc: 0.9715\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 12s 236us/step - loss: 0.1257 - acc: 0.9811 - val_loss: 0.1507 - val_acc: 0.9745\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 12s 235us/step - loss: 0.1157 - acc: 0.9835 - val_loss: 0.1496 - val_acc: 0.9755\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Main file\n",
    "\"\"\"\n",
    "\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.backend.set_session(sess)\n",
    "\n",
    "# Imports\n",
    "build_model = __import__('1-input').build_model\n",
    "optimize_model = __import__('2-optimize').optimize_model\n",
    "one_hot = __import__('3-one_hot').one_hot\n",
    "#train_model = __import__('5-train').train_model\n",
    "\n",
    "\n",
    "def train_model(network, data, labels, batch_size, epochs,\n",
    "                validation_data=None, verbose=True, shuffle=False):\n",
    "    \"\"\" doc \"\"\"\n",
    "    return network.fit(data, labels,\n",
    "                       batch_size=batch_size,\n",
    "                       epochs=epochs,\n",
    "                       verbose=verbose,\n",
    "                       validation_data=validation_data,\n",
    "                       shuffle=shuffle)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_train = datasets['X_train']\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    Y_train = datasets['Y_train']\n",
    "    Y_train_oh = one_hot(Y_train)\n",
    "    X_valid = datasets['X_valid']\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "    Y_valid = datasets['Y_valid']\n",
    "    Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "    lambtha = 0.0001\n",
    "    keep_prob = 0.95\n",
    "    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "    alpha = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    optimize_model(network, alpha, beta1, beta2)\n",
    "    batch_size = 64\n",
    "    epochs = 5\n",
    "    train_model(network, X_train, Y_train_oh, batch_size, epochs, validation_data=(X_valid, Y_valid_oh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "50000/50000 [==============================] - 13s 262us/step - loss: 0.3316 - acc: 0.9203 - val_loss: 0.1924 - val_acc: 0.9608\n",
      "Epoch 2/30\n",
      "41024/50000 [=======================>......] - ETA: 2s - loss: 0.1771 - acc: 0.9647"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9754acaa7969>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m     train_model(network, X_train, Y_train_oh, batch_size, epochs,\n\u001b[1;32m     50\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_valid_oh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 patience=3)\n\u001b[0m",
      "\u001b[0;32m~/hb/holbertonschool-machine_learning/supervised_learning/0x06-keras/6-train.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(network, data, labels, batch_size, epochs, validation_data, early_stopping, patience, verbose, shuffle)\u001b[0m\n\u001b[1;32m     23\u001b[0m                        \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                        \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                        shuffle=shuffle)\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Main file\n",
    "\"\"\"\n",
    "\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.backend.set_session(sess)\n",
    "\n",
    "# Imports\n",
    "build_model = __import__('1-input').build_model\n",
    "optimize_model = __import__('2-optimize').optimize_model\n",
    "one_hot = __import__('3-one_hot').one_hot\n",
    "train_model = __import__('6-train').train_model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_train = datasets['X_train']\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    Y_train = datasets['Y_train']\n",
    "    Y_train_oh = one_hot(Y_train)\n",
    "    X_valid = datasets['X_valid']\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "    Y_valid = datasets['Y_valid']\n",
    "    Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "    lambtha = 0.0001\n",
    "    keep_prob = 0.95\n",
    "    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "    alpha = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    optimize_model(network, alpha, beta1, beta2)\n",
    "    batch_size = 64\n",
    "    epochs = 30\n",
    "    train_model(network, X_train, Y_train_oh, batch_size, epochs,\n",
    "                validation_data=(X_valid, Y_valid_oh), early_stopping=True,\n",
    "                patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 1/1000\n",
      "50000/50000 [==============================] - 14s 278us/step - loss: 0.3323 - acc: 0.9186 - val_loss: 0.1923 - val_acc: 0.9600\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "Epoch 2/1000\n",
      "22080/50000 [============>.................] - ETA: 6s - loss: 0.1701 - acc: 0.9672"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-80de72462875>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m     train_model(network, X_train, Y_train_oh, batch_size, epochs,\n\u001b[1;32m     71\u001b[0m                 \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_valid_oh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 patience=3, learning_rate_decay=True, alpha=alpha)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-80de72462875>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(network, data, labels, batch_size, epochs, validation_data, early_stopping, patience, learning_rate_decay, alpha, decay_rate, verbose, shuffle)\u001b[0m\n\u001b[1;32m     47\u001b[0m                        \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                        \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                        shuffle=shuffle)\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mdatasets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/MNIST.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Main file\n",
    "\"\"\"\n",
    "\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.backend.set_session(sess)\n",
    "\n",
    "# Imports\n",
    "build_model = __import__('1-input').build_model\n",
    "optimize_model = __import__('2-optimize').optimize_model\n",
    "one_hot = __import__('3-one_hot').one_hot\n",
    "#train_model = __import__('7-train').train_model \n",
    "def train_model(network, data, labels, batch_size,\n",
    "                epochs, validation_data=None,\n",
    "                early_stopping=False, patience=0,\n",
    "                learning_rate_decay=False,\n",
    "                alpha=0.1, decay_rate=1,\n",
    "                verbose=True, shuffle=False):\n",
    "    \"\"\" doc \"\"\"\n",
    "    callbacks = None\n",
    "    if validation_data:\n",
    "        callbacks = [K.callbacks.EarlyStopping(monitor=\"loss\",\n",
    "                                               patience=patience,\n",
    "                                               mode=\"auto\")]\n",
    "        \n",
    "        def scheduler(epoch):\n",
    "            return alpha/(1+(decay_rate*(epoch)))\n",
    "        callbacks.append(K.callbacks.LearningRateScheduler(scheduler, 1))\n",
    "    return network.fit(data, labels,\n",
    "                       batch_size=batch_size,\n",
    "                       epochs=epochs,\n",
    "                       verbose=verbose,\n",
    "                       callbacks=callbacks,\n",
    "                       validation_data=validation_data,\n",
    "                       shuffle=shuffle)\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_train = datasets['X_train']\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    Y_train = datasets['Y_train']\n",
    "    Y_train_oh = one_hot(Y_train)\n",
    "    X_valid = datasets['X_valid']\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "    Y_valid = datasets['Y_valid']\n",
    "    Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "    lambtha = 0.0001\n",
    "    keep_prob = 0.95\n",
    "    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "    alpha = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    optimize_model(network, alpha, beta1, beta2)\n",
    "    batch_size = 64\n",
    "    epochs = 1000\n",
    "    train_model(network, X_train, Y_train_oh, batch_size, epochs,\n",
    "                validation_data=(X_valid, Y_valid_oh), early_stopping=True,\n",
    "                patience=3, learning_rate_decay=True, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.001.\n",
      "Epoch 1/1000\n",
      "50000/50000 [==============================] - 17s 349us/step - loss: 0.3311 - acc: 0.9191 - val_loss: 0.1866 - val_acc: 0.9625\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.0005.\n",
      "Epoch 2/1000\n",
      "50000/50000 [==============================] - 13s 257us/step - loss: 0.1608 - acc: 0.9686 - val_loss: 0.1484 - val_acc: 0.9732\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.0003333333333333333.\n",
      "Epoch 3/1000\n",
      "50000/50000 [==============================] - 13s 255us/step - loss: 0.1262 - acc: 0.9794 - val_loss: 0.1362 - val_acc: 0.9768\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00025.\n",
      "Epoch 4/1000\n",
      "50000/50000 [==============================] - 13s 262us/step - loss: 0.1095 - acc: 0.9839 - val_loss: 0.1275 - val_acc: 0.9786\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 5/1000\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.0988 - acc: 0.9875 - val_loss: 0.1250 - val_acc: 0.9794\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.00016666666666666666.\n",
      "Epoch 6/1000\n",
      "50000/50000 [==============================] - 13s 260us/step - loss: 0.0915 - acc: 0.9893 - val_loss: 0.1228 - val_acc: 0.9795\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.00014285714285714287.\n",
      "Epoch 7/1000\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.0855 - acc: 0.9913 - val_loss: 0.1210 - val_acc: 0.9788\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.000125.\n",
      "Epoch 8/1000\n",
      "50000/50000 [==============================] - 13s 262us/step - loss: 0.0818 - acc: 0.9923 - val_loss: 0.1190 - val_acc: 0.9799\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.00011111111111111112.\n",
      "Epoch 9/1000\n",
      "50000/50000 [==============================] - 13s 263us/step - loss: 0.0771 - acc: 0.9937 - val_loss: 0.1203 - val_acc: 0.9792\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 10/1000\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.0749 - acc: 0.9940 - val_loss: 0.1170 - val_acc: 0.9797\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 9.090909090909092e-05.\n",
      "Epoch 11/1000\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.0726 - acc: 0.9946 - val_loss: 0.1156 - val_acc: 0.9796\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 8.333333333333333e-05.\n",
      "Epoch 12/1000\n",
      "50000/50000 [==============================] - 13s 263us/step - loss: 0.0705 - acc: 0.9955 - val_loss: 0.1134 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 7.692307692307693e-05.\n",
      "Epoch 13/1000\n",
      "50000/50000 [==============================] - 13s 263us/step - loss: 0.0683 - acc: 0.9960 - val_loss: 0.1126 - val_acc: 0.9805\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 7.142857142857143e-05.\n",
      "Epoch 14/1000\n",
      "50000/50000 [==============================] - 13s 258us/step - loss: 0.0669 - acc: 0.9962 - val_loss: 0.1123 - val_acc: 0.9808\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 6.666666666666667e-05.\n",
      "Epoch 15/1000\n",
      "50000/50000 [==============================] - 13s 259us/step - loss: 0.0653 - acc: 0.9962 - val_loss: 0.1101 - val_acc: 0.9818\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 6.25e-05.\n",
      "Epoch 16/1000\n",
      "50000/50000 [==============================] - 13s 260us/step - loss: 0.0639 - acc: 0.9969 - val_loss: 0.1112 - val_acc: 0.9810\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 5.882352941176471e-05.\n",
      "Epoch 17/1000\n",
      "50000/50000 [==============================] - 13s 262us/step - loss: 0.0630 - acc: 0.9972 - val_loss: 0.1091 - val_acc: 0.9813\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 5.555555555555556e-05.\n",
      "Epoch 18/1000\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.0620 - acc: 0.9971 - val_loss: 0.1094 - val_acc: 0.9811\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 5.2631578947368424e-05.\n",
      "Epoch 19/1000\n",
      "50000/50000 [==============================] - 13s 262us/step - loss: 0.0606 - acc: 0.9975 - val_loss: 0.1082 - val_acc: 0.9813\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 5e-05.\n",
      "Epoch 20/1000\n",
      "50000/50000 [==============================] - 13s 262us/step - loss: 0.0598 - acc: 0.9978 - val_loss: 0.1083 - val_acc: 0.9816\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 4.761904761904762e-05.\n",
      "Epoch 21/1000\n",
      "50000/50000 [==============================] - 13s 262us/step - loss: 0.0595 - acc: 0.9973 - val_loss: 0.1063 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 4.545454545454546e-05.\n",
      "Epoch 22/1000\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.0580 - acc: 0.9980 - val_loss: 0.1068 - val_acc: 0.9815\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 4.347826086956522e-05.\n",
      "Epoch 23/1000\n",
      "50000/50000 [==============================] - 13s 265us/step - loss: 0.0581 - acc: 0.9976 - val_loss: 0.1056 - val_acc: 0.9816\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 4.1666666666666665e-05.\n",
      "Epoch 24/1000\n",
      "50000/50000 [==============================] - 13s 264us/step - loss: 0.0570 - acc: 0.9983 - val_loss: 0.1049 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 4e-05.\n",
      "Epoch 25/1000\n",
      "50000/50000 [==============================] - 13s 263us/step - loss: 0.0560 - acc: 0.9983 - val_loss: 0.1040 - val_acc: 0.9821\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 3.846153846153846e-05.\n",
      "Epoch 26/1000\n",
      "50000/50000 [==============================] - 13s 263us/step - loss: 0.0560 - acc: 0.9980 - val_loss: 0.1042 - val_acc: 0.9814\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 3.7037037037037037e-05.\n",
      "Epoch 27/1000\n",
      "50000/50000 [==============================] - 13s 267us/step - loss: 0.0554 - acc: 0.9983 - val_loss: 0.1031 - val_acc: 0.9818\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 3.571428571428572e-05.\n",
      "Epoch 28/1000\n",
      "50000/50000 [==============================] - 13s 262us/step - loss: 0.0549 - acc: 0.9983 - val_loss: 0.1037 - val_acc: 0.9822\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 3.4482758620689657e-05.\n",
      "Epoch 29/1000\n",
      "50000/50000 [==============================] - 13s 263us/step - loss: 0.0545 - acc: 0.9983 - val_loss: 0.1034 - val_acc: 0.9821\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 3.3333333333333335e-05.\n",
      "Epoch 30/1000\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.0540 - acc: 0.9985 - val_loss: 0.1030 - val_acc: 0.9815\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 3.2258064516129034e-05.\n",
      "Epoch 31/1000\n",
      "50000/50000 [==============================] - 13s 258us/step - loss: 0.0537 - acc: 0.9984 - val_loss: 0.1015 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 3.125e-05.\n",
      "Epoch 32/1000\n",
      "50000/50000 [==============================] - 13s 262us/step - loss: 0.0533 - acc: 0.9986 - val_loss: 0.1025 - val_acc: 0.9820\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 3.0303030303030302e-05.\n",
      "Epoch 33/1000\n",
      "50000/50000 [==============================] - 13s 263us/step - loss: 0.0529 - acc: 0.9986 - val_loss: 0.1012 - val_acc: 0.9821\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 2.9411764705882354e-05.\n",
      "Epoch 34/1000\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.0526 - acc: 0.9986 - val_loss: 0.1013 - val_acc: 0.9820\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 2.857142857142857e-05.\n",
      "Epoch 35/1000\n",
      "50000/50000 [==============================] - 13s 260us/step - loss: 0.0521 - acc: 0.9987 - val_loss: 0.1006 - val_acc: 0.9823\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 2.777777777777778e-05.\n",
      "Epoch 36/1000\n",
      "50000/50000 [==============================] - 13s 263us/step - loss: 0.0517 - acc: 0.9986 - val_loss: 0.0999 - val_acc: 0.9824\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 2.7027027027027027e-05.\n",
      "Epoch 37/1000\n",
      "50000/50000 [==============================] - 13s 258us/step - loss: 0.0515 - acc: 0.9987 - val_loss: 0.1000 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 2.6315789473684212e-05.\n",
      "Epoch 38/1000\n",
      "50000/50000 [==============================] - 13s 262us/step - loss: 0.0509 - acc: 0.9987 - val_loss: 0.0996 - val_acc: 0.9831\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 2.5641025641025643e-05.\n",
      "Epoch 39/1000\n",
      "50000/50000 [==============================] - 13s 262us/step - loss: 0.0509 - acc: 0.9987 - val_loss: 0.1002 - val_acc: 0.9823\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 2.5e-05.\n",
      "Epoch 40/1000\n",
      "50000/50000 [==============================] - 13s 258us/step - loss: 0.0503 - acc: 0.9989 - val_loss: 0.1000 - val_acc: 0.9827\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 2.4390243902439026e-05.\n",
      "Epoch 41/1000\n",
      "50000/50000 [==============================] - 13s 257us/step - loss: 0.0504 - acc: 0.9988 - val_loss: 0.0997 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 2.380952380952381e-05.\n",
      "Epoch 42/1000\n",
      "50000/50000 [==============================] - 13s 259us/step - loss: 0.0499 - acc: 0.9988 - val_loss: 0.0990 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 2.3255813953488374e-05.\n",
      "Epoch 43/1000\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.0500 - acc: 0.9987 - val_loss: 0.0992 - val_acc: 0.9827\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 2.272727272727273e-05.\n",
      "Epoch 44/1000\n",
      "50000/50000 [==============================] - 13s 264us/step - loss: 0.0491 - acc: 0.9991 - val_loss: 0.0988 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 2.2222222222222223e-05.\n",
      "Epoch 45/1000\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.0496 - acc: 0.9990 - val_loss: 0.0990 - val_acc: 0.9822\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 2.173913043478261e-05.\n",
      "Epoch 46/1000\n",
      "50000/50000 [==============================] - 13s 260us/step - loss: 0.0491 - acc: 0.9988 - val_loss: 0.0986 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 2.1276595744680852e-05.\n",
      "Epoch 47/1000\n",
      "50000/50000 [==============================] - 13s 258us/step - loss: 0.0488 - acc: 0.9989 - val_loss: 0.0986 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 2.0833333333333333e-05.\n",
      "Epoch 48/1000\n",
      "50000/50000 [==============================] - 13s 261us/step - loss: 0.0484 - acc: 0.9991 - val_loss: 0.0982 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 2.0408163265306123e-05.\n",
      "Epoch 49/1000\n",
      "50000/50000 [==============================] - 13s 265us/step - loss: 0.0481 - acc: 0.9992 - val_loss: 0.0982 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 2e-05.\n",
      "Epoch 50/1000\n",
      "50000/50000 [==============================] - 14s 271us/step - loss: 0.0483 - acc: 0.9989 - val_loss: 0.0979 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 1.9607843137254903e-05.\n",
      "Epoch 51/1000\n",
      "50000/50000 [==============================] - 13s 252us/step - loss: 0.0479 - acc: 0.9991 - val_loss: 0.0974 - val_acc: 0.9831\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 1.923076923076923e-05.\n",
      "Epoch 52/1000\n",
      "50000/50000 [==============================] - 13s 250us/step - loss: 0.0476 - acc: 0.9991 - val_loss: 0.0978 - val_acc: 0.9829\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 1.8867924528301888e-05.\n",
      "Epoch 53/1000\n",
      "50000/50000 [==============================] - 13s 251us/step - loss: 0.0477 - acc: 0.9990 - val_loss: 0.0972 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 1.8518518518518518e-05.\n",
      "Epoch 54/1000\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 0.0474 - acc: 0.9992 - val_loss: 0.0970 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 1.8181818181818182e-05.\n",
      "Epoch 55/1000\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 0.0471 - acc: 0.9993 - val_loss: 0.0967 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 1.785714285714286e-05.\n",
      "Epoch 56/1000\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 0.0469 - acc: 0.9992 - val_loss: 0.0969 - val_acc: 0.9827\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 1.7543859649122806e-05.\n",
      "Epoch 57/1000\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 0.0467 - acc: 0.9992 - val_loss: 0.0965 - val_acc: 0.9824\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 1.7241379310344828e-05.\n",
      "Epoch 58/1000\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 0.0470 - acc: 0.9990 - val_loss: 0.0960 - val_acc: 0.9827\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 1.694915254237288e-05.\n",
      "Epoch 59/1000\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 0.0467 - acc: 0.9990 - val_loss: 0.0964 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 1.6666666666666667e-05.\n",
      "Epoch 60/1000\n",
      "50000/50000 [==============================] - 12s 245us/step - loss: 0.0465 - acc: 0.9992 - val_loss: 0.0963 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 1.639344262295082e-05.\n",
      "Epoch 61/1000\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 0.0464 - acc: 0.9991 - val_loss: 0.0960 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 1.6129032258064517e-05.\n",
      "Epoch 62/1000\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 0.0465 - acc: 0.9990 - val_loss: 0.0957 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 1.5873015873015872e-05.\n",
      "Epoch 63/1000\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 0.0462 - acc: 0.9993 - val_loss: 0.0955 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 1.5625e-05.\n",
      "Epoch 64/1000\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 0.0458 - acc: 0.9992 - val_loss: 0.0961 - val_acc: 0.9829\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 1.5384615384615384e-05.\n",
      "Epoch 65/1000\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 0.0456 - acc: 0.9991 - val_loss: 0.0952 - val_acc: 0.9836\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 1.5151515151515151e-05.\n",
      "Epoch 66/1000\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 0.0457 - acc: 0.9991 - val_loss: 0.0958 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 1.4925373134328359e-05.\n",
      "Epoch 67/1000\n",
      "50000/50000 [==============================] - 12s 245us/step - loss: 0.0456 - acc: 0.9993 - val_loss: 0.0956 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 1.4705882352941177e-05.\n",
      "Epoch 68/1000\n",
      "50000/50000 [==============================] - 13s 250us/step - loss: 0.0454 - acc: 0.9991 - val_loss: 0.0955 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 1.4492753623188407e-05.\n",
      "Epoch 69/1000\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 0.0454 - acc: 0.9993 - val_loss: 0.0960 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 1.4285714285714285e-05.\n",
      "Epoch 70/1000\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 0.0454 - acc: 0.9991 - val_loss: 0.0951 - val_acc: 0.9827\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 1.4084507042253522e-05.\n",
      "Epoch 71/1000\n",
      "50000/50000 [==============================] - 13s 252us/step - loss: 0.0451 - acc: 0.9992 - val_loss: 0.0950 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 1.388888888888889e-05.\n",
      "Epoch 72/1000\n",
      "50000/50000 [==============================] - 13s 252us/step - loss: 0.0451 - acc: 0.9993 - val_loss: 0.0942 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 1.3698630136986302e-05.\n",
      "Epoch 73/1000\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 0.0449 - acc: 0.9992 - val_loss: 0.0948 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 1.3513513513513513e-05.\n",
      "Epoch 74/1000\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 0.0446 - acc: 0.9995 - val_loss: 0.0945 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 1.3333333333333333e-05.\n",
      "Epoch 75/1000\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 0.0445 - acc: 0.9994 - val_loss: 0.0946 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 1.3157894736842106e-05.\n",
      "Epoch 76/1000\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 0.0446 - acc: 0.9992 - val_loss: 0.0945 - val_acc: 0.9826\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 1.2987012987012988e-05.\n",
      "Epoch 77/1000\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 0.0443 - acc: 0.9994 - val_loss: 0.0940 - val_acc: 0.9831\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 1.2820512820512822e-05.\n",
      "Epoch 78/1000\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 0.0442 - acc: 0.9994 - val_loss: 0.0941 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 1.2658227848101267e-05.\n",
      "Epoch 79/1000\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 0.0446 - acc: 0.9992 - val_loss: 0.0944 - val_acc: 0.9831\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 1.25e-05.\n",
      "Epoch 80/1000\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 0.0441 - acc: 0.9993 - val_loss: 0.0944 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 1.234567901234568e-05.\n",
      "Epoch 81/1000\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 0.0442 - acc: 0.9994 - val_loss: 0.0936 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 1.2195121951219513e-05.\n",
      "Epoch 82/1000\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 0.0442 - acc: 0.9990 - val_loss: 0.0938 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 1.2048192771084337e-05.\n",
      "Epoch 83/1000\n",
      "50000/50000 [==============================] - 12s 245us/step - loss: 0.0440 - acc: 0.9991 - val_loss: 0.0937 - val_acc: 0.9829\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 1.1904761904761905e-05.\n",
      "Epoch 84/1000\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 0.0438 - acc: 0.9994 - val_loss: 0.0939 - val_acc: 0.9831\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 1.1764705882352942e-05.\n",
      "Epoch 85/1000\n",
      "50000/50000 [==============================] - 13s 254us/step - loss: 0.0440 - acc: 0.9992 - val_loss: 0.0936 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 1.1627906976744187e-05.\n",
      "Epoch 86/1000\n",
      "50000/50000 [==============================] - 12s 245us/step - loss: 0.0438 - acc: 0.9993 - val_loss: 0.0933 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 1.1494252873563218e-05.\n",
      "Epoch 87/1000\n",
      "50000/50000 [==============================] - 12s 245us/step - loss: 0.0437 - acc: 0.9994 - val_loss: 0.0931 - val_acc: 0.9834\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 1.1363636363636365e-05.\n",
      "Epoch 88/1000\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 0.0436 - acc: 0.9993 - val_loss: 0.0934 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 1.1235955056179776e-05.\n",
      "Epoch 89/1000\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 0.0433 - acc: 0.9994 - val_loss: 0.0928 - val_acc: 0.9831\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 1.1111111111111112e-05.\n",
      "Epoch 90/1000\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 0.0432 - acc: 0.9996 - val_loss: 0.0931 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 1.0989010989010989e-05.\n",
      "Epoch 91/1000\n",
      "50000/50000 [==============================] - 12s 246us/step - loss: 0.0433 - acc: 0.9994 - val_loss: 0.0933 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 1.0869565217391305e-05.\n",
      "Epoch 92/1000\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 0.0432 - acc: 0.9993 - val_loss: 0.0929 - val_acc: 0.9828\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 1.0752688172043012e-05.\n",
      "Epoch 93/1000\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 0.0434 - acc: 0.9992 - val_loss: 0.0930 - val_acc: 0.9831\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 1.0638297872340426e-05.\n",
      "Epoch 94/1000\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 0.0428 - acc: 0.9994 - val_loss: 0.0929 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 1.0526315789473684e-05.\n",
      "Epoch 95/1000\n",
      "50000/50000 [==============================] - 13s 250us/step - loss: 0.0431 - acc: 0.9994 - val_loss: 0.0927 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 1.0416666666666666e-05.\n",
      "Epoch 96/1000\n",
      "50000/50000 [==============================] - 12s 247us/step - loss: 0.0427 - acc: 0.9995 - val_loss: 0.0927 - val_acc: 0.9830\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 1.0309278350515464e-05.\n",
      "Epoch 97/1000\n",
      "50000/50000 [==============================] - 12s 248us/step - loss: 0.0430 - acc: 0.9993 - val_loss: 0.0929 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 1.0204081632653061e-05.\n",
      "Epoch 98/1000\n",
      "50000/50000 [==============================] - 12s 250us/step - loss: 0.0428 - acc: 0.9993 - val_loss: 0.0928 - val_acc: 0.9833\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 1.0101010101010101e-05.\n",
      "Epoch 99/1000\n",
      "50000/50000 [==============================] - 12s 249us/step - loss: 0.0428 - acc: 0.9992 - val_loss: 0.0928 - val_acc: 0.9828\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Main file\n",
    "\"\"\"\n",
    "\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.backend.set_session(sess)\n",
    "\n",
    "# Imports\n",
    "build_model = __import__('1-input').build_model\n",
    "optimize_model = __import__('2-optimize').optimize_model\n",
    "one_hot = __import__('3-one_hot').one_hot\n",
    "#train_model = __import__('8-train').train_model \n",
    "def train_model(network, data, labels, batch_size,\n",
    "                epochs, validation_data=None,\n",
    "                early_stopping=False, patience=0,\n",
    "                learning_rate_decay=False, alpha=0.1,\n",
    "                decay_rate=1, save_best=False, filepath=None,\n",
    "                verbose=True, shuffle=False):\n",
    "    \"\"\" doc \"\"\"\n",
    "    callbacks = None\n",
    "    if validation_data:\n",
    "        callbacks = [K.callbacks.EarlyStopping(monitor=\"loss\",\n",
    "                                               patience=patience,\n",
    "                                               mode=\"auto\")]\n",
    "\n",
    "        def scheduler(epoch):\n",
    "            \"\"\" new learning \"\"\"\n",
    "            return alpha/(1+(decay_rate*(epoch)))\n",
    "        callbacks.append(K.callbacks.LearningRateScheduler(scheduler, 1))\n",
    "        callbacks.append(K.callbacks.ModelCheckpoint(filepath,\n",
    "                                                     monitor='val_loss',\n",
    "                                                     save_best_only=save_best,\n",
    "                                                     mode='auto'))\n",
    "\n",
    "    return network.fit(data, labels,\n",
    "                       batch_size=batch_size,\n",
    "                       epochs=epochs,\n",
    "                       verbose=verbose,\n",
    "                       callbacks=callbacks,\n",
    "                       validation_data=validation_data,\n",
    "                       shuffle=shuffle)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_train = datasets['X_train']\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    Y_train = datasets['Y_train']\n",
    "    Y_train_oh = one_hot(Y_train)\n",
    "    X_valid = datasets['X_valid']\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "    Y_valid = datasets['Y_valid']\n",
    "    Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "    lambtha = 0.0001\n",
    "    keep_prob = 0.95\n",
    "    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "    alpha = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    optimize_model(network, alpha, beta1, beta2)\n",
    "    batch_size = 64\n",
    "    epochs = 1000\n",
    "    train_model(network, X_train, Y_train_oh, batch_size, epochs,\n",
    "                validation_data=(X_valid, Y_valid_oh), early_stopping=True,\n",
    "                patience=3, learning_rate_decay=True, alpha=alpha,\n",
    "                save_best=True, filepath='network1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
