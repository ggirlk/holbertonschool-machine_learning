{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cp 4-train.py 5-train.py\n",
    "#!touch m.py\n",
    "!chmod +x *.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_197 (Dense)            (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dropout_104 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_198 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_105 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_199 (Dense)            (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[<tf.Tensor 'kernel/Regularizer_149/add:0' shape=() dtype=float32>, <tf.Tensor 'kernel/Regularizer_150/add:0' shape=() dtype=float32>, <tf.Tensor 'kernel/Regularizer_151/add:0' shape=() dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "#build_model = __import__('0-sequential').build_model\n",
    "\"\"\" doc \"\"\"\n",
    "import tensorflow.keras as K\n",
    "\n",
    "\n",
    "def build_model(nx, layers, activations, lambtha, keep_prob):\n",
    "    \"\"\" doc \"\"\"\n",
    "    model = K.Sequential()\n",
    "    for i in range(len(layers)):\n",
    "        init = K.initializers.VarianceScaling(mode=\"fan_avg\")\n",
    "        # freg = K.layers.ActivityRegularization(l2=lambtha)\n",
    "        freg = K.regularizers.l2(lambtha)\n",
    "        layer = K.layers.Dense(layers[i], input_dim=nx,\n",
    "                               activation=activations[i],\n",
    "                               kernel_initializer=init,\n",
    "                               kernel_regularizer=freg)\n",
    "        model.add(layer)\n",
    "        if i != len(layers)-1:\n",
    "            dropped = K.layers.Dropout(rate=1-keep_prob)\n",
    "            model.add(dropped)\n",
    "    return (model)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    network = build_model(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "    network.summary()\n",
    "    print(network.losses)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "[<tf.Tensor 'dense/kernel/Regularizer/add:0' shape=() dtype=float32>, <tf.Tensor 'dense_1/kernel/Regularizer/add:0' shape=() dtype=float32>, <tf.Tensor 'dense_2/kernel/Regularizer/add:0' shape=() dtype=float32>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_103 (InputLayer)       (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_409 (Dense)            (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dropout_309 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_410 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_310 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_411 (Dense)            (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 269,322\n",
      "Trainable params: 269,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[<tf.Tensor 'kernel/Regularizer_276/add:0' shape=() dtype=float32>, <tf.Tensor 'kernel/Regularizer_277/add:0' shape=() dtype=float32>, <tf.Tensor 'kernel/Regularizer_278/add:0' shape=() dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "def build_model(nx, layers, activations, lambtha, keep_prob):\n",
    "    inputs = K.Input(shape=(nx,))\n",
    "    freg = K.regularizers.l2(lambtha)\n",
    "    x = K.layers.Dense(layers[0], activation=activations[0],\n",
    "                           kernel_regularizer=freg)(inputs)\n",
    "    for i in range(1, len(layers)):\n",
    "        x = K.layers.Dropout(rate=1-keep_prob)(x)\n",
    "        x = K.layers.Dense(layers[i], activation=activations[i],\n",
    "                           kernel_regularizer=freg)(x)\n",
    "        \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "    return (model)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    network = build_model(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "    network.summary()\n",
    "    print(network.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical_crossentropy\n",
      "['accuracy']\n",
      "<class 'tensorflow.python.keras.optimizers.Adam'>\n",
      "(0.01, 0.99, 0.9)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "build_model = __import__('1-input').build_model\n",
    "# optimize_model = __import__('2-optimize').optimize_model\n",
    "def optimize_model(network, alpha, beta1, beta2):\n",
    "    \"\"\" doc \"\"\"\n",
    "    optimizer = K.optimizers.Adam(alpha, beta1, beta2)\n",
    "    network.compile(optimizer,\n",
    "                    loss=\"categorical_crossentropy\",\n",
    "                    metrics=['accuracy'])\n",
    "    return None\n",
    "if __name__ == '__main__':\n",
    "    model = build_model(784, [256, 256, 10], ['tanh', 'tanh', 'softmax'], 0.001, 0.95)\n",
    "    optimize_model(model, 0.01, 0.99, 0.9)\n",
    "    print(model.loss)\n",
    "    print(model.metrics)\n",
    "    opt = model.optimizer\n",
    "    print(opt.__class__)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(sess.run((opt.lr, opt.beta_1, opt.beta_2))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 0 4 1 9 2 1 3 1 4]\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import numpy as np\n",
    "#one_hot = __import__('3-one_hot').one_hot\n",
    "def one_hot(labels, classes=None):\n",
    "    \"\"\" doc \"\"\"\n",
    "    return K.utils.to_categorical(labels, classes)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    labels = np.load('../data/MNIST.npz')['Y_train'][:10]\n",
    "    print(labels)\n",
    "    print(one_hot(labels))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 11s 225us/step - loss: 0.3327 - acc: 0.9195\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 0.1763 - acc: 0.9650\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 11s 215us/step - loss: 0.1434 - acc: 0.9750\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 11s 216us/step - loss: 0.1257 - acc: 0.9804\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 11s 217us/step - loss: 0.1167 - acc: 0.9824\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Main file\n",
    "\"\"\"\n",
    "\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.backend.set_session(sess)\n",
    "\n",
    "# Imports\n",
    "build_model = __import__('1-input').build_model\n",
    "optimize_model = __import__('2-optimize').optimize_model\n",
    "one_hot = __import__('3-one_hot').one_hot\n",
    "train_model = __import__('4-train').train_model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_train = datasets['X_train']\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    Y_train = datasets['Y_train']\n",
    "    Y_train_oh = one_hot(Y_train)\n",
    "\n",
    "    lambtha = 0.0001\n",
    "    keep_prob = 0.95\n",
    "    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "    alpha = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    optimize_model(network, alpha, beta1, beta2)\n",
    "    batch_size = 64\n",
    "    epochs = 5\n",
    "    train_model(network, X_train, Y_train_oh, batch_size, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 13s 253us/step - loss: 0.3306 - acc: 0.9185 - val_loss: 0.1900 - val_acc: 0.9603\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 12s 232us/step - loss: 0.1770 - acc: 0.9657 - val_loss: 0.1579 - val_acc: 0.9698\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 12s 234us/step - loss: 0.1431 - acc: 0.9756 - val_loss: 0.1586 - val_acc: 0.9715\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 12s 236us/step - loss: 0.1257 - acc: 0.9811 - val_loss: 0.1507 - val_acc: 0.9745\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 12s 235us/step - loss: 0.1157 - acc: 0.9835 - val_loss: 0.1496 - val_acc: 0.9755\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Main file\n",
    "\"\"\"\n",
    "\n",
    "# Force Seed - fix for Keras\n",
    "SEED = 0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(SEED)\n",
    "import tensorflow.keras as K\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.backend.set_session(sess)\n",
    "\n",
    "# Imports\n",
    "build_model = __import__('1-input').build_model\n",
    "optimize_model = __import__('2-optimize').optimize_model\n",
    "one_hot = __import__('3-one_hot').one_hot\n",
    "#train_model = __import__('5-train').train_model\n",
    "\n",
    "\n",
    "def train_model(network, data, labels, batch_size, epochs,\n",
    "                validation_data=None, verbose=True, shuffle=False):\n",
    "    \"\"\" doc \"\"\"\n",
    "    return network.fit(data, labels,\n",
    "                       batch_size=batch_size,\n",
    "                       epochs=epochs,\n",
    "                       verbose=verbose,\n",
    "                       validation_data=validation_data,\n",
    "                       shuffle=shuffle)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    datasets = np.load('../data/MNIST.npz')\n",
    "    X_train = datasets['X_train']\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    Y_train = datasets['Y_train']\n",
    "    Y_train_oh = one_hot(Y_train)\n",
    "    X_valid = datasets['X_valid']\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], -1)\n",
    "    Y_valid = datasets['Y_valid']\n",
    "    Y_valid_oh = one_hot(Y_valid)\n",
    "\n",
    "    lambtha = 0.0001\n",
    "    keep_prob = 0.95\n",
    "    network = build_model(784, [256, 256, 10], ['relu', 'relu', 'softmax'], lambtha, keep_prob)\n",
    "    alpha = 0.001\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    optimize_model(network, alpha, beta1, beta2)\n",
    "    batch_size = 64\n",
    "    epochs = 5\n",
    "    train_model(network, X_train, Y_train_oh, batch_size, epochs, validation_data=(X_valid, Y_valid_oh))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1/5\n",
    "50000/50000 [==============================] - 12s 247us/step - loss: 0.3294 - acc: 0.9195\n",
    "Epoch 2/5\n",
    "50000/50000 [==============================] - 10s 202us/step - loss: 0.1775 - acc: 0.9650\n",
    "Epoch 3/5\n",
    "50000/50000 [==============================] - 11s 226us/step - loss: 0.1429 - acc: 0.9756\n",
    "Epoch 4/5\n",
    "50000/50000 [==============================] - 11s 222us/step - loss: 0.1252 - acc: 0.9808\n",
    "Epoch 5/5\n",
    "50000/50000 [==============================] - 11s 224us/step - loss: 0.1162 - acc: 0.9832Train on 50000 samples, validate on 10000 samples\n",
    "Epoch 1/5\n",
    "50000/50000 [==============================] - 13s 254us/step - loss: 0.3288 - acc: 0.9192 - val_loss: 0.1860 - val_acc: 0.9660\n",
    "Epoch 2/5\n",
    "50000/50000 [==============================] - 12s 232us/step - loss: 0.1752 - acc: 0.9652 - val_loss: 0.1594 - val_acc: 0.9697\n",
    "Epoch 3/5\n",
    "50000/50000 [==============================] - 12s 231us/step - loss: 0.1419 - acc: 0.9756 - val_loss: 0.1558 - val_acc: 0.9709\n",
    "Epoch 4/5\n",
    "50000/50000 [==============================] - 12s 233us/step - loss: 0.1243 - acc: 0.9804 - val_loss: 0.1461 - val_acc: 0.9743\n",
    "Epoch 5/5\n",
    "50000/50000 [==============================] - 12s 231us/step - loss: 0.1147 - acc: 0.9833 - val_loss: 0.1460 - val_acc: 0.9751"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
