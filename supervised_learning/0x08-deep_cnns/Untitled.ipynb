{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/khouloud/hb/holbertonschool-machine_learning/supervised_learning/0x08-deep_cnns\n"
     ]
    }
   ],
   "source": [
    "#!cp 1-inception_network.py 4-resnet50.py\n",
    "#!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 224, 224, 96) 384         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 224, 224, 16) 64          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 224, 224, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 224, 224, 64) 256         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 224, 224, 128 110720      conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 224, 224, 32) 12832       conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 224, 224, 32) 128         max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 224, 224, 256 0           conv2d[0][0]                     \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 124,384\n",
      "Trainable params: 124,384\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import tensorflow.keras as K\n",
    "#inception_block = __import__('0-inception_block').inception_block\n",
    "def inception_block(A_prev, filters):\n",
    "    \"\"\" doc \"\"\"\n",
    "    MaxPooling2D = K.layers.MaxPooling2D\n",
    "    Conv2D = K.layers.Conv2D\n",
    "    Concatenate = K.layers.Concatenate()\n",
    "    \n",
    "    F1, F3R, F3, F5R, F5, FPP = filters\n",
    "    \n",
    "    layer1x1_0 = Conv2D(F1, 1, activation='relu')(A_prev) #\n",
    "    \n",
    "    layer1x1_1 = Conv2D(F3R, 1, padding='same', activation='relu')(A_prev)\n",
    "    \n",
    "    layer3x3 = Conv2D(F3, 3, padding='same', activation='relu')(layer1x1_1) #\n",
    "    \n",
    "    layer1x1_2 = Conv2D(F5R, 1, padding='same', activation='relu')(A_prev)\n",
    "    \n",
    "    layer5x5 = Conv2D(F5, 5, padding='same',  activation='relu')(layer1x1_2) #\n",
    "    \n",
    "    layerMax = MaxPooling2D(1)(A_prev)\n",
    "    \n",
    "    layer1x1_3 = Conv2D(FPP, 1, padding='same', activation='relu')(layerMax) #\n",
    "    \n",
    "    layer_out = Concatenate([layer1x1_0, layer3x3, layer5x5, layer1x1_3])\n",
    "    return layer_out\n",
    "if __name__ == '__main__':\n",
    "    X = K.Input(shape=(224, 224, 3))\n",
    "    Y = inception_block(X, [64, 96, 128, 16, 32, 32])\n",
    "    model = K.models.Model(inputs=X, outputs=Y)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " [(None, 224, 224, 64), (None, 222, 222, 128), (None, 220, 220, 32), (None, 224, 224, 32)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# one by one from the table\n",
    "def inception_network():\n",
    "    \"\"\" doc \"\"\"\n",
    "    MaxPooling2D = K.layers.MaxPooling2D\n",
    "    AveragePooling2D = K.layers.AveragePooling2D\n",
    "    Conv2D = K.layers.Conv2D\n",
    "    Dropout = K.layers.Dropout(rate=0.6)\n",
    "    Dense = K.layers.Dense\n",
    "    X = K.Input(shape=(224, 224, 3))\n",
    "    \n",
    "    layer7x7 = Conv2D(64, 7, 2, padding='same',  activation='relu')(X)\n",
    "    layerMax = MaxPooling2D(3, 2, padding='same')(layer7x7)\n",
    "    \n",
    "    layer3x3 = Conv2D(64, 1, 1,  activation='relu')(layerMax)\n",
    "    layer3x3 = Conv2D(192, 3, 1, padding='same',  activation='relu')(layer3x3)\n",
    "    layerMax_2 = MaxPooling2D(3, 2, padding='same')(layer3x3)\n",
    "\n",
    "    inception = inception_block(layerMax_2, [64, 96, 128, 16, 32, 32])\n",
    "    inception = inception_block(inception, [128, 128, 192, 32, 96, 64])\n",
    "    \n",
    "    layerMax_2 = MaxPooling2D(3, 2, padding='same')(inception)\n",
    "    inception = inception_block(layerMax_2, [192, 96, 208, 16, 48, 64])\n",
    "    inception = inception_block(inception, [160, 112, 224, 24, 64, 64])\n",
    "    inception = inception_block(inception, [128, 128, 256, 24, 64, 64])\n",
    "    inception = inception_block(inception, [112, 144, 288, 32, 64, 64])\n",
    "    inception = inception_block(inception, [256, 160, 320, 32, 128, 128])\n",
    "    layerMax_2 = MaxPooling2D(3, 2, padding='same')(inception)\n",
    "    inception = inception_block(layerMax_2, [256, 160, 320, 32, 128, 128])\n",
    "    inception = inception_block(inception, [384, 192, 384, 48, 128, 128])\n",
    "    layerAVG_2 = AveragePooling2D(7, 1)(inception)\n",
    "    dropped = Dropout(layerAVG_2)\n",
    "    # dropped = Dense(1000, activation=\"relu\")(dropped)\n",
    "    Y = Dense(1000, activation=\"softmax\")(dropped)\n",
    "\n",
    "    model = K.Model(inputs=X, outputs=Y)\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = inception_network()\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 224, 224, 256 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 224, 224, 64) 16448       input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 224, 224, 64) 256         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 224, 224, 64) 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 224, 224, 64) 36928       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 224, 224, 64) 256         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 224, 224, 64) 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 224, 224, 256 16640       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 224, 224, 256 1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 224, 224, 256 0           batch_normalization_8[0][0]      \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 224, 224, 256 0           add_2[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 71,552\n",
      "Trainable params: 70,784\n",
      "Non-trainable params: 768\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def identity_block(A_prev, filters):\n",
    "    \"\"\" doc \"\"\"\n",
    "    Conv2D = K.layers.Conv2D\n",
    "    BatchNorm = K.layers.BatchNormalization\n",
    "    Activation = K.layers.Activation\n",
    "    Add = K.layers.Add\n",
    "    F11, F3, F12 = filters\n",
    "    layer1x1 = Conv2D(F11, 1, padding='same', kernel_initializer='he_normal')(A_prev)\n",
    "    layer1x1 = BatchNorm()(layer1x1)\n",
    "    layer1x1 = Activation('relu')(layer1x1)\n",
    "\n",
    "    layer3x3 = Conv2D(F3, 3, padding='same', kernel_initializer='he_normal')(layer1x1)\n",
    "    layer3x3 = BatchNorm()(layer3x3)\n",
    "    layer3x3 = Activation('relu')(layer3x3)\n",
    "\n",
    "    layer1x1 = Conv2D(F12, 1, padding='same', kernel_initializer='he_normal')(layer3x3)\n",
    "    layer1x1 = BatchNorm()(layer1x1)\n",
    "\n",
    "    layer_out = Add()([layer1x1, A_prev])\n",
    "    layer_out = Activation('relu')(layer_out)\n",
    "\n",
    "    return layer_out\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    X = K.Input(shape=(224, 224, 256))\n",
    "    Y = identity_block(X, [64, 64, 256])\n",
    "    model = K.models.Model(inputs=X, outputs=Y)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 112, 112, 64) 256         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 112, 112, 64) 256         conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 112, 112, 64) 0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 112, 112, 64) 36928       activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 112, 112, 64) 256         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 112, 112, 64) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 112, 112, 256 16640       activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 112, 112, 256 1024        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 112, 112, 256 1024        conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 112, 112, 256 1024        conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 112, 112, 256 0           batch_normalization_2[0][0]      \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 112, 112, 256 0           add[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 57,408\n",
      "Trainable params: 56,128\n",
      "Non-trainable params: 1,280\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "projection_block = __import__('3-projection_block').projection_block\n",
    "\n",
    "def projection_block(A_prev, filters, s=2):\n",
    "    \"\"\" doc \"\"\"\n",
    "    Conv2D = K.layers.Conv2D\n",
    "    BatchNorm = K.layers.BatchNormalization\n",
    "    Activation = K.layers.Activation\n",
    "    Add = K.layers.Add\n",
    "    F11, F3, F12 = filters\n",
    "\n",
    "    layer1x1 = Conv2D(F11, 1, s, padding='same',\n",
    "                      kernel_initializer='he_normal')(A_prev)\n",
    "    layer1x1 = BatchNorm()(layer1x1)\n",
    "    layer1x1 = Activation('relu')(layer1x1)\n",
    "\n",
    "    layer3x3 = Conv2D(F3, 3, padding='same',\n",
    "                      kernel_initializer='he_normal')(layer1x1)\n",
    "    layer3x3 = BatchNorm()(layer3x3)\n",
    "    layer3x3 = Activation('relu')(layer3x3)\n",
    "\n",
    "    layer1x1 = Conv2D(F12, 1, padding='same',\n",
    "                      kernel_initializer='he_normal')(layer3x3)\n",
    "    layer1x1 = BatchNorm()(layer1x1)  #\n",
    "    \n",
    "    layer1x1_s = Conv2D(F12, 1, s, padding='same',\n",
    "                      kernel_initializer='he_normal')(A_prev)\n",
    "    layer1x1_s = BatchNorm()(layer1x1_s)  #\n",
    "\n",
    "    layer_out = Add()([layer1x1, layer1x1_s])\n",
    "    layer_out = Activation('relu')(layer_out)\n",
    "\n",
    "    return layer_out\n",
    "if __name__ == '__main__':\n",
    "    X = K.Input(shape=(224, 224, 3))\n",
    "    Y = projection_block(X, [64, 64, 256])\n",
    "    model = K.models.Model(inputs=X, outputs=Y)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_650 (Conv2D)             (None, 112, 112, 64) 9472        input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_650 (BatchN (None, 112, 112, 64) 256         conv2d_650[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_602 (Activation)     (None, 112, 112, 64) 0           batch_normalization_650[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_651 (Conv2D)             (None, 56, 56, 64)   4160        activation_602[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_651 (BatchN (None, 56, 56, 64)   256         conv2d_651[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_603 (Activation)     (None, 56, 56, 64)   0           batch_normalization_651[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_652 (Conv2D)             (None, 56, 56, 64)   36928       activation_603[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_652 (BatchN (None, 56, 56, 64)   256         conv2d_652[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_604 (Activation)     (None, 56, 56, 64)   0           batch_normalization_652[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_653 (Conv2D)             (None, 56, 56, 256)  16640       activation_604[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_654 (Conv2D)             (None, 56, 56, 256)  16640       activation_602[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_653 (BatchN (None, 56, 56, 256)  1024        conv2d_653[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_654 (BatchN (None, 56, 56, 256)  1024        conv2d_654[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_193 (Add)                   (None, 56, 56, 256)  0           batch_normalization_653[0][0]    \n",
      "                                                                 batch_normalization_654[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_605 (Activation)     (None, 56, 56, 256)  0           add_193[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_655 (Conv2D)             (None, 56, 56, 64)   16448       activation_605[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_655 (BatchN (None, 56, 56, 64)   256         conv2d_655[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_606 (Activation)     (None, 56, 56, 64)   0           batch_normalization_655[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_656 (Conv2D)             (None, 56, 56, 64)   36928       activation_606[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_656 (BatchN (None, 56, 56, 64)   256         conv2d_656[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_607 (Activation)     (None, 56, 56, 64)   0           batch_normalization_656[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_657 (Conv2D)             (None, 56, 56, 256)  16640       activation_607[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_657 (BatchN (None, 56, 56, 256)  1024        conv2d_657[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_194 (Add)                   (None, 56, 56, 256)  0           batch_normalization_657[0][0]    \n",
      "                                                                 activation_605[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_608 (Activation)     (None, 56, 56, 256)  0           add_194[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_658 (Conv2D)             (None, 56, 56, 64)   16448       activation_608[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_658 (BatchN (None, 56, 56, 64)   256         conv2d_658[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_609 (Activation)     (None, 56, 56, 64)   0           batch_normalization_658[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_659 (Conv2D)             (None, 56, 56, 64)   36928       activation_609[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_659 (BatchN (None, 56, 56, 64)   256         conv2d_659[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_610 (Activation)     (None, 56, 56, 64)   0           batch_normalization_659[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_660 (Conv2D)             (None, 56, 56, 256)  16640       activation_610[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_660 (BatchN (None, 56, 56, 256)  1024        conv2d_660[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_195 (Add)                   (None, 56, 56, 256)  0           batch_normalization_660[0][0]    \n",
      "                                                                 activation_608[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_611 (Activation)     (None, 56, 56, 256)  0           add_195[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_661 (Conv2D)             (None, 28, 28, 128)  32896       activation_611[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_661 (BatchN (None, 28, 28, 128)  512         conv2d_661[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_612 (Activation)     (None, 28, 28, 128)  0           batch_normalization_661[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_662 (Conv2D)             (None, 28, 28, 128)  147584      activation_612[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_662 (BatchN (None, 28, 28, 128)  512         conv2d_662[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_613 (Activation)     (None, 28, 28, 128)  0           batch_normalization_662[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_663 (Conv2D)             (None, 28, 28, 512)  66048       activation_613[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_664 (Conv2D)             (None, 28, 28, 512)  131584      activation_611[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_663 (BatchN (None, 28, 28, 512)  2048        conv2d_663[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_664 (BatchN (None, 28, 28, 512)  2048        conv2d_664[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_196 (Add)                   (None, 28, 28, 512)  0           batch_normalization_663[0][0]    \n",
      "                                                                 batch_normalization_664[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_614 (Activation)     (None, 28, 28, 512)  0           add_196[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_665 (Conv2D)             (None, 28, 28, 128)  65664       activation_614[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_665 (BatchN (None, 28, 28, 128)  512         conv2d_665[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_615 (Activation)     (None, 28, 28, 128)  0           batch_normalization_665[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_666 (Conv2D)             (None, 28, 28, 128)  147584      activation_615[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_666 (BatchN (None, 28, 28, 128)  512         conv2d_666[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_616 (Activation)     (None, 28, 28, 128)  0           batch_normalization_666[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_667 (Conv2D)             (None, 28, 28, 512)  66048       activation_616[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_667 (BatchN (None, 28, 28, 512)  2048        conv2d_667[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_197 (Add)                   (None, 28, 28, 512)  0           batch_normalization_667[0][0]    \n",
      "                                                                 activation_614[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_617 (Activation)     (None, 28, 28, 512)  0           add_197[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_668 (Conv2D)             (None, 28, 28, 128)  65664       activation_617[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_668 (BatchN (None, 28, 28, 128)  512         conv2d_668[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_618 (Activation)     (None, 28, 28, 128)  0           batch_normalization_668[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_669 (Conv2D)             (None, 28, 28, 128)  147584      activation_618[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_669 (BatchN (None, 28, 28, 128)  512         conv2d_669[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_619 (Activation)     (None, 28, 28, 128)  0           batch_normalization_669[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_670 (Conv2D)             (None, 28, 28, 512)  66048       activation_619[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_670 (BatchN (None, 28, 28, 512)  2048        conv2d_670[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_198 (Add)                   (None, 28, 28, 512)  0           batch_normalization_670[0][0]    \n",
      "                                                                 activation_617[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_620 (Activation)     (None, 28, 28, 512)  0           add_198[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_671 (Conv2D)             (None, 28, 28, 128)  65664       activation_620[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_671 (BatchN (None, 28, 28, 128)  512         conv2d_671[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_621 (Activation)     (None, 28, 28, 128)  0           batch_normalization_671[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_672 (Conv2D)             (None, 28, 28, 128)  147584      activation_621[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_672 (BatchN (None, 28, 28, 128)  512         conv2d_672[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_622 (Activation)     (None, 28, 28, 128)  0           batch_normalization_672[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_673 (Conv2D)             (None, 28, 28, 512)  66048       activation_622[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_673 (BatchN (None, 28, 28, 512)  2048        conv2d_673[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_199 (Add)                   (None, 28, 28, 512)  0           batch_normalization_673[0][0]    \n",
      "                                                                 activation_620[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_623 (Activation)     (None, 28, 28, 512)  0           add_199[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_674 (Conv2D)             (None, 14, 14, 256)  131328      activation_623[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_674 (BatchN (None, 14, 14, 256)  1024        conv2d_674[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_624 (Activation)     (None, 14, 14, 256)  0           batch_normalization_674[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_675 (Conv2D)             (None, 14, 14, 256)  590080      activation_624[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_675 (BatchN (None, 14, 14, 256)  1024        conv2d_675[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_625 (Activation)     (None, 14, 14, 256)  0           batch_normalization_675[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_676 (Conv2D)             (None, 14, 14, 1024) 263168      activation_625[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_677 (Conv2D)             (None, 14, 14, 1024) 525312      activation_623[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_676 (BatchN (None, 14, 14, 1024) 4096        conv2d_676[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_677 (BatchN (None, 14, 14, 1024) 4096        conv2d_677[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_200 (Add)                   (None, 14, 14, 1024) 0           batch_normalization_676[0][0]    \n",
      "                                                                 batch_normalization_677[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_626 (Activation)     (None, 14, 14, 1024) 0           add_200[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_678 (Conv2D)             (None, 14, 14, 256)  262400      activation_626[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_678 (BatchN (None, 14, 14, 256)  1024        conv2d_678[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_627 (Activation)     (None, 14, 14, 256)  0           batch_normalization_678[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_679 (Conv2D)             (None, 14, 14, 256)  590080      activation_627[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_679 (BatchN (None, 14, 14, 256)  1024        conv2d_679[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_628 (Activation)     (None, 14, 14, 256)  0           batch_normalization_679[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_680 (Conv2D)             (None, 14, 14, 1024) 263168      activation_628[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_680 (BatchN (None, 14, 14, 1024) 4096        conv2d_680[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_201 (Add)                   (None, 14, 14, 1024) 0           batch_normalization_680[0][0]    \n",
      "                                                                 activation_626[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_629 (Activation)     (None, 14, 14, 1024) 0           add_201[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_681 (Conv2D)             (None, 14, 14, 256)  262400      activation_629[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_681 (BatchN (None, 14, 14, 256)  1024        conv2d_681[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_630 (Activation)     (None, 14, 14, 256)  0           batch_normalization_681[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_682 (Conv2D)             (None, 14, 14, 256)  590080      activation_630[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_682 (BatchN (None, 14, 14, 256)  1024        conv2d_682[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_631 (Activation)     (None, 14, 14, 256)  0           batch_normalization_682[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_683 (Conv2D)             (None, 14, 14, 1024) 263168      activation_631[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_683 (BatchN (None, 14, 14, 1024) 4096        conv2d_683[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_202 (Add)                   (None, 14, 14, 1024) 0           batch_normalization_683[0][0]    \n",
      "                                                                 activation_629[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_632 (Activation)     (None, 14, 14, 1024) 0           add_202[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_684 (Conv2D)             (None, 14, 14, 256)  262400      activation_632[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_684 (BatchN (None, 14, 14, 256)  1024        conv2d_684[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_633 (Activation)     (None, 14, 14, 256)  0           batch_normalization_684[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_685 (Conv2D)             (None, 14, 14, 256)  590080      activation_633[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_685 (BatchN (None, 14, 14, 256)  1024        conv2d_685[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_634 (Activation)     (None, 14, 14, 256)  0           batch_normalization_685[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_686 (Conv2D)             (None, 14, 14, 1024) 263168      activation_634[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_686 (BatchN (None, 14, 14, 1024) 4096        conv2d_686[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_203 (Add)                   (None, 14, 14, 1024) 0           batch_normalization_686[0][0]    \n",
      "                                                                 activation_632[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_635 (Activation)     (None, 14, 14, 1024) 0           add_203[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_687 (Conv2D)             (None, 14, 14, 256)  262400      activation_635[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_687 (BatchN (None, 14, 14, 256)  1024        conv2d_687[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_636 (Activation)     (None, 14, 14, 256)  0           batch_normalization_687[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_688 (Conv2D)             (None, 14, 14, 256)  590080      activation_636[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_688 (BatchN (None, 14, 14, 256)  1024        conv2d_688[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_637 (Activation)     (None, 14, 14, 256)  0           batch_normalization_688[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_689 (Conv2D)             (None, 14, 14, 1024) 263168      activation_637[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_689 (BatchN (None, 14, 14, 1024) 4096        conv2d_689[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_204 (Add)                   (None, 14, 14, 1024) 0           batch_normalization_689[0][0]    \n",
      "                                                                 activation_635[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_638 (Activation)     (None, 14, 14, 1024) 0           add_204[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_690 (Conv2D)             (None, 7, 7, 512)    524800      activation_638[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_690 (BatchN (None, 7, 7, 512)    2048        conv2d_690[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_639 (Activation)     (None, 7, 7, 512)    0           batch_normalization_690[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_691 (Conv2D)             (None, 7, 7, 512)    2359808     activation_639[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_691 (BatchN (None, 7, 7, 512)    2048        conv2d_691[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_640 (Activation)     (None, 7, 7, 512)    0           batch_normalization_691[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_692 (Conv2D)             (None, 7, 7, 2048)   1050624     activation_640[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_693 (Conv2D)             (None, 7, 7, 2048)   2099200     activation_638[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_692 (BatchN (None, 7, 7, 2048)   8192        conv2d_692[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_693 (BatchN (None, 7, 7, 2048)   8192        conv2d_693[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_205 (Add)                   (None, 7, 7, 2048)   0           batch_normalization_692[0][0]    \n",
      "                                                                 batch_normalization_693[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_641 (Activation)     (None, 7, 7, 2048)   0           add_205[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_694 (Conv2D)             (None, 7, 7, 512)    1049088     activation_641[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_694 (BatchN (None, 7, 7, 512)    2048        conv2d_694[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_642 (Activation)     (None, 7, 7, 512)    0           batch_normalization_694[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_695 (Conv2D)             (None, 7, 7, 512)    2359808     activation_642[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_695 (BatchN (None, 7, 7, 512)    2048        conv2d_695[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_643 (Activation)     (None, 7, 7, 512)    0           batch_normalization_695[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_696 (Conv2D)             (None, 7, 7, 2048)   1050624     activation_643[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_696 (BatchN (None, 7, 7, 2048)   8192        conv2d_696[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_206 (Add)                   (None, 7, 7, 2048)   0           batch_normalization_696[0][0]    \n",
      "                                                                 activation_641[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_644 (Activation)     (None, 7, 7, 2048)   0           add_206[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_697 (Conv2D)             (None, 7, 7, 512)    1049088     activation_644[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_697 (BatchN (None, 7, 7, 512)    2048        conv2d_697[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_645 (Activation)     (None, 7, 7, 512)    0           batch_normalization_697[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_698 (Conv2D)             (None, 7, 7, 512)    2359808     activation_645[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_698 (BatchN (None, 7, 7, 512)    2048        conv2d_698[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_646 (Activation)     (None, 7, 7, 512)    0           batch_normalization_698[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_699 (Conv2D)             (None, 7, 7, 2048)   1050624     activation_646[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_699 (BatchN (None, 7, 7, 2048)   8192        conv2d_699[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_207 (Add)                   (None, 7, 7, 2048)   0           batch_normalization_699[0][0]    \n",
      "                                                                 activation_644[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_647 (Activation)     (None, 7, 7, 2048)   0           add_207[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_12 (AveragePo (None, 7, 7, 2048)   0           activation_647[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 7, 7, 1000)   2049000     average_pooling2d_12[0][0]       \n",
      "==================================================================================================\n",
      "Total params: 24,514,920\n",
      "Trainable params: 24,464,872\n",
      "Non-trainable params: 50,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "identity_block = __import__('2-identity_block').identity_block\n",
    "projection_block = __import__('3-projection_block').projection_block\n",
    "\n",
    "\n",
    "\n",
    "def resnet50():\n",
    "    \"\"\" doc \"\"\"\n",
    "    MaxPooling2D = K.layers.MaxPooling2D\n",
    "    AveragePooling2D = K.layers.AveragePooling2D\n",
    "    Conv2D = K.layers.Conv2D\n",
    "    Dropout = K.layers.Dropout(rate=0.4)\n",
    "    Dense = K.layers.Dense\n",
    "    BatchNorm = K.layers.BatchNormalization\n",
    "    Activation = K.layers.Activation\n",
    "    Add = K.layers.Add\n",
    "    def layersConv(X, l, f, s=None, p='same'):\n",
    "        layer = Conv2D(l, f, s, padding=p,\n",
    "                       kernel_initializer='he_normal')(X)\n",
    "        layer = BatchNorm()(layer)\n",
    "        layer = Activation('relu')(layer)\n",
    "        return layer\n",
    "\n",
    "    X = K.Input(shape=(224, 224, 3))\n",
    "\n",
    "    layer = layersConv(X, 64, 7, 2)\n",
    "\n",
    "    layerMax = MaxPooling2D(3, 1, padding='same')(layer)\n",
    "\n",
    "    layer = projection_block(layer, [64, 64, 256])\n",
    "    layer = identity_block(layer, [64, 64, 256])\n",
    "    layer = identity_block(layer, [64, 64, 256])\n",
    "\n",
    "    layer = projection_block(layer, [128, 128, 512])\n",
    "    layer = identity_block(layer, [128, 128, 512])\n",
    "    layer = identity_block(layer, [128, 128, 512])\n",
    "    layer = identity_block(layer, [128, 128, 512])\n",
    "\n",
    "    layer = projection_block(layer, [256, 256, 1024])\n",
    "    layer = identity_block(layer, [256, 256, 1024])\n",
    "    layer = identity_block(layer, [256, 256, 1024])\n",
    "    layer = identity_block(layer, [256, 256, 1024])\n",
    "    layer = identity_block(layer, [256, 256, 1024])\n",
    "\n",
    "    layer = projection_block(layer, [512, 512, 2048])\n",
    "    layer = identity_block(layer, [512, 512, 2048])\n",
    "    layer = identity_block(layer, [512, 512, 2048])\n",
    "\n",
    "    layerAVG = AveragePooling2D(1, 1)(layer)\n",
    "\n",
    "    Y = Dense(1000, activation=\"softmax\")(layerAVG)\n",
    "\n",
    "    model = K.Model(inputs=X, outputs=Y)\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = resnet50()\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
